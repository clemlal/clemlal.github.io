<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematical Methods for Supervised Learning M1 SID (Toulouse University, 2025-2026)</title>
    <link rel="stylesheet" href="../../css/styles.css">
</head>
<body>
    <div class="container">
        <main class="main">
            <div id="toptitle">
                <h1>Mathematical Methods for Supervised Learning M1 SID (2024-2025)</h1>
                <strong>Lecturer:</strong> <a href="../../index.html" target="_blank" rel="noopener noreferrer">Clément Lalanne</a>
            </div>
            <div id="content">
                <h2>Overview</h2>
                <p>This course introduces mathematical methods essential for supervised learning. Topics include linear models, optimization techniques, and advanced methods like kernel and sparse methods. Students are expected to have basic knowledge of linear algebra, calculus, and probability.</p>
            </div>

            <h2>Evaluation</h2>
            <p>The class will be evaluated by a final exam (70%) and three inverted class activities (les rendus des cours inversés) (30%)</p>

            <h2>Inverted class guidelines</h2>
            <p>Les quatre derniers cours de cette classe seront dispensés sous la forme de cours inversés. <br>
Concrètement, avant chaque séance, il vous sera demandé d'étudier des documents mis à disposition (en général une ou deux vidéos, et éventuellement un chapitre de textbook), puis de remplir une feuille (template disponible ci-dessous).
<br>
Cette feuille consiste à élaborer le plan détaillé d'un cours hypothétique que vous donneriez vous-même sur le sujet étudié. Ces documents seront ramassés au début de la séance : pensez donc à les prendre en photo avant de les rendre, afin d'en conserver une copie pour les activités qui auront lieu en classe.
<br>
Exception : pour le premier cours inversé, je ne vous demanderai pas de rendre la feuille complétée ; nous la remplirons ensemble en classe, afin de clarifier les attentes et la méthode.
<br>
Si vous souhaitez un exemple de ce qui est attendu par « plan détaillé », vous pouvez vous inspirer de ce qui est demandé à l'oral de l'agrégation de mathématiques, où un exercice très proche est proposé :  <a href="https://perso.eleves.ens-rennes.fr/people/Marie.Derrien/agregation.html#nav" target="_blank" rel="noopener noreferrer">Exemples</a></p>

            <h2>Lectures</h2>
            <ul>
                <li><strong>Lecture 1</strong>  <a href="SupervisedLearningSIDM1_Introduction_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Introduction to Supervised Learning</a>.</li>
                <li><strong>Lecture 2</strong>  <a href="SupervisedLearningSIDM1_Introduction_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Introduction to Supervised Learning</a> continued and  <a href="SupervisedLearningSIDM1_LinearModels_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Linear models</a>.</li>
                <li><strong>Lecture 3</strong> <a href="SupervisedLearningSIDM1_LinearModels_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Linear models</a>.</li>
                <li><strong>Lecture 4</strong> <a href="SupervisedLearningSIDM1_KernelMethods_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Kernel Methods</a> (<a href="https://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/notes/aronszajn.pdf" target="_blank" rel="noopener noreferrer">Jean-Philippe Vert's proof of Aronszajn's theorem</a>).</li>
                <li><strong>Lecture 5</strong>  Classification and regression trees (Documents to prepare : <a href="https://www.youtube.com/watch?v=_L39rN6gz7Y" target="_blank" rel="noopener noreferrer">Video 1</a>, <a href="https://www.youtube.com/watch?v=J4Wdy0Wc_xQ" target="_blank" rel="noopener noreferrer">Video 2</a>)</li>
                <li><strong>Lecture 6</strong> </li>
                <li><strong>Lecture 7</strong> </li>
                <li><strong>Lecture 8</strong> </li>
            </ul>

            <h2>TDs</h2>
            <ul>
                <li><strong>TD 1</strong>: </li>
                <li><strong>TD 2</strong>: </li>
                <li><strong>TD 3</strong>: </li>
                <li><strong>TD 4</strong>: </li>
            </ul>
            Some of the original material was made by <a href="https://www.math.univ-toulouse.fr/~fbachoc/" target="_blank" rel="noopener noreferrer">François Bachoc</a> and by <a href="https://perso.math.univ-toulouse.fr/amazoyer/" target="_blank" rel="noopener noreferrer">Adrien Mazoyer</a>.

            <h2>TPs</h2>
            <ul>
                <li><strong>TP 1</strong>: </li>
                <li><strong>TP 2</strong>: </li>
                <li><strong>TP 3</strong>: </li>
            </ul>

            <h2>References and External Resources</h2>

            <h3>Machine Learning and Learning Theory</h3>
            <ul>
                <li><a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target="_blank" rel="noopener noreferrer">Learning Theory from First Principles</a> by <a href="https://www.di.ens.fr/~fbach/" target="_blank" rel="noopener noreferrer">Francis Bach</a>, 2024: <strong>Main reference for the course.</strong></li>
                <li><a href="https://arxiv.org/abs/2404.17625" target="_blank" rel="noopener noreferrer">Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land</a> by Simone Scardapane, 2024: A very accessible hands on presentation of modern machine learning methods.</li> 
                <li><a href="https://cs.nyu.edu/~mohri/mlbook/" target="_blank" rel="noopener noreferrer">Foundations of Machine Learning</a> by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, 2018.</li>
            </ul>

            <h3>Generative AI</h3>
            <ul>
                <li><a href="https://www.youtube.com/@Deepia-ls2fo" target="_blank" rel="noopener noreferrer">Deepia's Youtube channel.</li>
                <li><a href="https://arxiv.org/pdf/2403.18103" target="_blank" rel="noopener noreferrer">Tutorial on Diffusion Models for Imaging and Vision</a> by Stanley Chan, 2024.</li>
                <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank" rel="noopener noreferrer">What are diffusion models ?</a> by Lilian Weng, 2021.</li>
                <li><a href="https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/" target="_blank" rel="noopener noreferrer">A Visual Dive into Conditional Flow Matching</a> by Gagneux et al., 2025.</li>
            </ul>

            <h3>Optimization for Machine Learning</h3>
            <ul>
                <li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener noreferrer">Convex Optimization</a> by Stephen Boyd and Lieven Vandenberghe, Cambridge University Press, 2012.</li>
                <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611974997" target="_blank" rel="noopener noreferrer">First-Order Optimization Methods</a> by Amir Beck, 2017.</li>
                <li><a href="https://arxiv.org/pdf/2101.09545" target="_blank" rel="noopener noreferrer">Acceleration Methods</a> by Alexandre d’Aspremont, Damien Scieur and Adrien Taylor, 2024.</li>
            </ul>

            <h3>Measure and Probability Theory</h3>
            <ul>
                <li><a href="https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/IPPA2.pdf" target="_blank" rel="noopener noreferrer">Intégration, Probabilités et Processus Aléatoires</a> by Jean-François Le Gall, 2006: An excellent introduction to measure theory with elements of stochastic processes and conditional expectation.</li>
                <li>Probabilités 2 by Jean-Yves Ouvrard, 2009: A detailed treatment of advanced topics in probability theory.</li>
                <li>Concentration Inequalities: A Nonasymptotic Theory of Independence by Stéphane Boucheron, Gábor Lugosi, and Pascal Massart, 2013.</li>
            </ul>

            <h3>Analysis</h3>
            <ul>
                <li><a href="http://www.cmap.polytechnique.fr/~massot/MAP431_web/MAP431_Histoire_Massot/Livres_Bony_Lutzen/Jean-Michel%20Bony-Cours%20d%27analyse.%20The%CC%81orie%20des%20distributions%20et%20analyse%20de%20Fourier-Ellipses(2001).pdf" target="_blank" rel="noopener noreferrer">Cours d'analyse</a> by Jean-Michel Bony, 2001.</li>
            </ul>

        </main>
    </div>
</body>
</html>
