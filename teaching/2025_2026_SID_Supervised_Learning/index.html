<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematical Methods for Supervised Learning M1 SID (Paul Sabatier, 2024-2025)</title>
    <link rel="stylesheet" href="../../css/styles.css">
</head>
<body>
    <div class="container">
        <main class="main">
            <div id="toptitle">
                <h1>Mathematical Methods for Supervised Learning M1 SID (2024-2025)</h1>
                <strong>Lecturer:</strong> <a href="../../index.html" target="_blank" rel="noopener noreferrer">Clément Lalanne</a>
            </div>
            <div id="content">
                <h2>Overview</h2>
                <p>This course introduces mathematical methods essential for supervised learning. Topics include linear models, optimization techniques, and advanced methods like kernel and sparse methods. Students are expected to have basic knowledge of linear algebra, calculus, and probability.</p>
            </div>

            <h2>Evaluation</h2>
            <p>The class will be evaluated by a final exam (70%) and three MCQs (QCM en Français) (30%)</p>

            <h2>Lectures</h2>
            <ul>
                <li><strong>Lecture 1</strong> </li>
                <li><strong>Lecture 2</strong> </li>
                <li><strong>Lecture 3</strong> </li>
                <li><strong>Lecture 4</strong> </li>
                <li><strong>Lecture 5</strong> </li>
                <li><strong>Lecture 6</strong> </li>
                <li><strong>Lecture 7</strong> </li>
                <li><strong>Lecture 8</strong> </li>
            </ul>

            <h2>TDs</h2>
            <ul>
                <li><strong>TD 1</strong>: </li>
                <li><strong>TD 2</strong>: </li>
                <li><strong>TD 3</strong>: </li>
                <li><strong>TD 4</strong>: </li>
            </ul>
            Some of the original material was made by <a href="https://www.math.univ-toulouse.fr/~fbachoc/" target="_blank" rel="noopener noreferrer">François Bachoc</a> and by <a href="https://perso.math.univ-toulouse.fr/amazoyer/" target="_blank" rel="noopener noreferrer">Adrien Mazoyer</a>.

            <h2>TPs</h2>
            <ul>
                <li><strong>TP 1</strong>: </li>
                <li><strong>TP 2</strong>: </li>
                <li><strong>TP 3</strong>: </li>
            </ul>

            <h2>References and External Resources</h2>

            <h3>Machine Learning and Learning Theory</h3>
            <ul>
                <li><a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target="_blank" rel="noopener noreferrer">Learning Theory from First Principles</a> by <a href="https://www.di.ens.fr/~fbach/" target="_blank" rel="noopener noreferrer">Francis Bach</a>, 2024: <strong>Main reference for the course.</strong></li>
                <li><a href="https://arxiv.org/abs/2404.17625" target="_blank" rel="noopener noreferrer">Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land</a> by Simone Scardapane, 2024: A very accessible hands on presentation of modern machine learning methods.</li> 
                <li><a href="https://cs.nyu.edu/~mohri/mlbook/" target="_blank" rel="noopener noreferrer">Foundations of Machine Learning</a> by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, 2018.</li>
            </ul>

            <h3>Optimization for Machine Learning</h3>
            <ul>
                <li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener noreferrer">Convex Optimization</a> by Stephen Boyd and Lieven Vandenberghe, Cambridge University Press, 2012.</li>
                <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611974997" target="_blank" rel="noopener noreferrer">First-Order Optimization Methods</a> by Amir Beck, 2017.</li>
                <li><a href="https://arxiv.org/pdf/2101.09545" target="_blank" rel="noopener noreferrer">Acceleration Methods</a> by Alexandre d’Aspremont, Damien Scieur and Adrien Taylor, 2024.</li>
            </ul>

            <h3>Measure and Probability Theory</h3>
            <ul>
                <li><a href="https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/IPPA2.pdf" target="_blank" rel="noopener noreferrer">Intégration, Probabilités et Processus Aléatoires</a> by Jean-François Le Gall, 2006: An excellent introduction to measure theory with elements of stochastic processes and conditional expectation.</li>
                <li>Probabilités 2 by Jean-Yves Ouvrard, 2009: A detailed treatment of advanced topics in probability theory.</li>
                <li>Concentration Inequalities: A Nonasymptotic Theory of Independence by Stéphane Boucheron, Gábor Lugosi, and Pascal Massart, 2013.</li>
            </ul>

            <h3>Analysis</h3>
            <ul>
                <li><a href="http://www.cmap.polytechnique.fr/~massot/MAP431_web/MAP431_Histoire_Massot/Livres_Bony_Lutzen/Jean-Michel%20Bony-Cours%20d%27analyse.%20The%CC%81orie%20des%20distributions%20et%20analyse%20de%20Fourier-Ellipses(2001).pdf" target="_blank" rel="noopener noreferrer">Cours d'analyse</a> by Jean-Michel Bony, 2001.</li>
            </ul>

        </main>
    </div>
</body>
</html>
