{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA4POD14LSpY"
   },
   "source": [
    "# Ensemble methods and Fairness\n",
    "This notebook aims at presenting some basics on ensemble methods and, hopefully, fairness. It is inspired by [the Google Python notebook on fairness](https://colab.research.google.com/notebooks/mlcc/intro_to_fairness.ipynb) that you might consult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zSWfNxgMKVW"
   },
   "source": [
    "## About the Dataset and Prediction Task\n",
    "\n",
    "In this exercise, you'll work with the [Adult Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income), which is commonly used in machine learning literature. This data was extracted from the [1994 Census bureau database](http://www.census.gov/en.html) by Ronny Kohavi and Barry Becker.\n",
    "\n",
    "Each example in the dataset contains the following demographic data for a set of individuals who took part in the 1994 Census:\n",
    "\n",
    "### Numeric Features\n",
    "*   `age`: The age of the individual in years.\n",
    "*   `fnlwgt`: The number of individuals the Census Organizations believes that set of observations represents.\n",
    "*   `education_num`:  An enumeration of the categorical representation of education. The higher the number, the higher the education that individual achieved. For example, an `education_num` of `11` represents `Assoc_voc` (associate degree at a vocational school), an `education_num` of `13` represents `Bachelors`, and an `education_num` of `9` represents `HS-grad` (high school graduate).\n",
    "*   `capital_gain`: Capital gain made by the individual, represented in US Dollars.\n",
    "*   `capital_loss`: Capital loss mabe by the individual, represented in US Dollars.\n",
    "*   `hours_per_week`: Hours worked per week.\n",
    "\n",
    "### Categorical Features\n",
    "*   `workclass`: The individual's type of employer. Examples include: `Private`, `Self-emp-not-inc`, `Self-emp-inc`, `Federal-gov`, `Local-gov`, `State-gov`, `Without-pay`, and `Never-worked`.\n",
    "*   `education`: The highest level of education achieved for that individual.\n",
    "*   `marital_status`: Marital status of the individual. Examples include: `Married-civ-spouse`, `Divorced`, `Never-married`, `Separated`, `Widowed`, `Married-spouse-absent`, and `Married-AF-spouse`.\n",
    "*   `occupation`: The occupation of the individual. Example include: `tech-support`, `Craft-repair`, `Other-service`, `Sales`, `Exec-managerial` and more.\n",
    "*   `relationship`:  The relationship of each individual in a household. Examples include: `Wife`, `Own-child`, `Husband`, `Not-in-family`, `Other-relative`, and `Unmarried`.\n",
    "*   `gender`:  Gender of the individual available only in binary choices: `Female` or `Male`.\n",
    "*   `race`: `White`, `Asian-Pac-Islander`, `Amer-Indian-Eskimo`, `Black`, and `Other`. \n",
    "*   `native_country`: Country of origin of the individual. Examples include: `United-States`, `Cambodia`, `England`, `Puerto-Rico`, `Canada`, `Germany`, `Outlying-US(Guam-USVI-etc)`, `India`, `Japan`, `United-States`, `Cambodia`, `England`, `Puerto-Rico`, `Canada`, `Germany`, `Outlying-US(Guam-USVI-etc)`, `India`, `Japan`, and more.\n",
    "\n",
    "### Prediction Task\n",
    "The prediction task is to **determine whether a person makes over $50,000 US Dollar a year.**\n",
    "\n",
    "### Label\n",
    "*   `income_bracket`: Whether the person makes more than $50,000 US Dollars annually.\n",
    "\n",
    "### Notes on Data Collection\n",
    "\n",
    "All the examples extracted for this dataset meet the following conditions: \n",
    "*   `age` is 16 years or older.\n",
    "*   The adjusted gross income (used to calculate `income_bracket`) is greater than $100 USD annually.\n",
    "*   `fnlwgt` is greater than 0.\n",
    "*   `hours_per_week` is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"not crashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SY8snnUKMSJE"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, import some modules that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "Iqqyi4v6MRWv",
    "outputId": "c7c1e381-b18e-40b5-c08f-9d3a858ce78c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "!pip install seaborn==0.8.1\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "#from google.colab import widgets\n",
    "# For facets\n",
    "from IPython.core.display import display, HTML\n",
    "import base64\n",
    "!pip install tf-nightly\n",
    "!pip install facets-overview==1.0.0\n",
    "from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator\n",
    "\n",
    "print('Modules are imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm9PvtV1NGJ-"
   },
   "source": [
    "### Load the Adult Dataset\n",
    "\n",
    "With the modules now imported, we can load the Adult dataset into a pandas DataFrame data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sXBfzcpbNGXs",
    "outputId": "16599c09-fefe-40ed-8153-88a4d2521ef1"
   },
   "outputs": [],
   "source": [
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "           \"income_bracket\"]\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "    names=COLUMNS,\n",
    "    sep=r'\\s*,\\s*',\n",
    "    engine='python',\n",
    "    na_values=\"?\")\n",
    "test_df = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "    names=COLUMNS,\n",
    "    sep=r'\\s*,\\s*',\n",
    "    skiprows=[0],\n",
    "    engine='python',\n",
    "    na_values=\"?\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_df = train_df.dropna(how=\"any\", axis=0)\n",
    "test_df = test_df.dropna(how=\"any\", axis=0)\n",
    "\n",
    "print('UCI Adult Census Income dataset loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1B-lFoejM6el"
   },
   "source": [
    "## Analyzing the Adult Dataset with Facets\n",
    "\n",
    "To start, we can use [Facets Overview](https://pair-code.github.io/facets/), an interactive visualization tool that can help us explore the dataset. With Facets Overview, we can quickly analyze the distribution of values across the Adult dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G9avfp-EMMC-",
    "outputId": "586ddeb0-1866-4d20-89ae-50b14b9b7e1c"
   },
   "outputs": [],
   "source": [
    "#@title Visualize the Data in Facets\n",
    "fsg = FeatureStatisticsGenerator()\n",
    "dataframes = [\n",
    "    {'table': train_df, 'name': 'trainData'}]\n",
    "censusProto = fsg.ProtoFromDataFrames(dataframes)\n",
    "protostr = base64.b64encode(censusProto.SerializeToString()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n",
    "        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n",
    "        <facets-overview id=\"elem\"></facets-overview>\n",
    "        <script>\n",
    "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(protostr=protostr)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3pf65z_NQxX"
   },
   "source": [
    "We drop some irrelevant variables (mostly zero valued): 'capital_gain', 'capital_loss' and we try to select the best depth in our DT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erRw5p9PAmwE"
   },
   "source": [
    "# Decision trees\n",
    "We recall the properties of a Decision Tree below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dqfMSkLAqXq"
   },
   "source": [
    "Decision trees, and in particular classification and regression trees (CART), are supervised estimators introduced by Leo Breiman et al.\n",
    "The paradigm of a binary decision tree is to recursively split the space $\\mathcal X$ with simple rules such that: is the explicative variable $x_j$ greater than the threshold $\\tau$ or not?\n",
    "Doing so, a decision tree is built, for which each node corresponds to a simple rule (and secondarly to a partition cell of $\\mathcal X$).\n",
    "The final result is a partition of $\\mathcal X$ by hypercubes.\n",
    "\n",
    "At each step of the learning algorithm, \n",
    "\n",
    "1. consider the partition $\\mathcal P = \\{\\mathcal X\\}$;\n",
    "1. for each cell $\\mathcal A$ of $\\mathcal P$, define the two-cell partition\n",
    "$\\mathcal A = \\mathcal L_{j, \\tau} \\cup \\mathcal R_{j, \\tau}$, where $j \\in [d]$ is a feature index and $\\tau \\in \\mathbb R$ is a threhold, and\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        \\mathcal L_{j, \\tau} = \\left\\{ x \\in \\mathcal A :  x_j \\le \\tau \\right\\}\\\\\n",
    "        \\mathcal R_{j, \\tau} = \\left\\{ x \\in \\mathcal A :  x_j > \\tau \\right\\}\n",
    "        %= \\mathcal A \\backslash \\mathcal L_{j, \\tau}.\n",
    "    \\end{cases}\n",
    "$$\n",
    "are the \"left\" and \"right\" parts of $\\mathcal A$.\n",
    "Then, find the best pair (feature, threhold) for splitting:\n",
    "$$\n",
    "    (j, \\tau) \\in \\operatorname{arg\\,min}_{1 \\le j \\le d \\atop \\tau \\in \\mathbb R}\n",
    "    \\frac{\\left| \\mathcal L_{j, \\tau} \\right|}{\\left| \\mathcal A \\right|} D(\\mathcal L_{j, \\tau}) +\n",
    "    \\frac{\\left| \\mathcal R_{j, \\tau} \\right|}{\\left| \\mathcal A \\right|} D(\\mathcal R_{j, \\tau})\n",
    "$$\n",
    "where $D$ is a distortion measure for a cell (see below);\n",
    "1. replace $\\mathcal A$ by $\\mathcal L_{j, \\tau}$ and $\\mathcal R_{j, \\tau}$ in the partition $\\mathcal P$;\n",
    "1. go to 2.\n",
    "\n",
    "Given a cell $\\mathcal A$, one may define the ratio of observations of $\\mathcal A$ of class $y \\in \\mathcal Y$:\n",
    "$$\n",
    "    p_y(\\mathcal A) = \\frac{\\left| \\left\\{ i \\in [n] : X_i \\in \\mathcal A, Y_i=y \\right\\} \\right|}{\\left| \\mathcal A \\right|}.\n",
    "$$\n",
    "\n",
    "Then, the distortion of the cell $\\mathcal A$ may be:\n",
    "- Gini impurity: $D(\\mathcal A) = \\sum_{y \\in \\mathcal Y} p_y(\\mathcal A) (1-p_y(\\mathcal A))$ (classification);\n",
    "- entropy: $D(\\mathcal A) = - \\sum_{y \\in \\mathcal Y} p_y(\\mathcal A) \\log(p_y(\\mathcal A))$ (classification);\n",
    "- mean squared error: $D(\\mathcal A) = \\frac{1}{\\left| \\mathcal A \\right|}\\sum_{1 \\le i \\le n \\atop X_i \\in \\mathcal A} \\left( Y_i - \\bar Y_{\\mathcal A} \\right)^2$, with $\\bar Y_{\\mathcal A} = \\frac{1}{\\left| \\mathcal A \\right|}\\sum_{1 \\le i \\le n \\atop X_i \\in \\mathcal A} Y_i$ (regression).\n",
    "\n",
    "For regression, Jerome Friedman suggested an improved criterion (in its original paper tackling gradient boosting), referred to as Friedman's mean squared error:\n",
    "$$\n",
    "    (j, \\tau) \\in \\operatorname{arg\\,min}_{1 \\le j \\le d \\atop \\tau \\in \\mathbb R}\n",
    "    \\frac{ \\left| \\mathcal L_{j, \\tau} \\right| \\left| \\mathcal R_{j, \\tau} \\right| }{ \\left| \\mathcal L_{j, \\tau} \\right| + \\left| \\mathcal R_{j, \\tau} \\right| }\n",
    "    \\left( \\bar Y_{\\mathcal L_{j, \\tau}} - \\bar Y_{\\mathcal R_{j, \\tau}} \\right)^2.\n",
    "$$\n",
    "\n",
    "Last but not least, several stopping rules are of interests:\n",
    "- maximal depth of the tree;\n",
    "- minimal number of observations required to split an internal node;\n",
    "- minimal number of observations required to be at a leaf node;\n",
    "- maximal number of leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgZ6dCTWAv3t"
   },
   "source": [
    "We would like to assess the accuracy of a [classification tree](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) with respect to the kind of splitting criterion (Gini impurity or entropy) and to the maximal depth allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "BFvbsaePTUYQ",
    "outputId": "38bffe1a-b501-4581-f07f-6cbbf2278067"
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "X_raw=train_df.drop(['income_bracket','capital_gain', 'capital_loss'], axis=1)\n",
    "y=train_df['income_bracket'].apply(lambda x: \">50K\" in x).astype(int)\n",
    "\n",
    "X_test_raw=test_df.drop(['income_bracket','capital_gain', 'capital_loss'], axis=1)\n",
    "y_test =test_df['income_bracket'].apply(lambda x: \">50K\" in x).astype(int)\n",
    "\n",
    "# Encode the categorical features as numbers\n",
    "def number_encode_features(df):\n",
    "    result = df.copy()\n",
    "    encoders = {}\n",
    "    for column in result.columns:\n",
    "        if result.dtypes[column] == np.object:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            result[column] = encoders[column].fit_transform(result[column])\n",
    "    return result, encoders\n",
    "\n",
    "X,encoders = number_encode_features(X_raw)\n",
    "\n",
    "X_train = X.to_numpy()\n",
    "y_train = y.to_numpy()\n",
    "\n",
    "X_test,encoders_test = number_encode_features(X_test_raw)\n",
    "\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "\n",
    "# testing the parameters \n",
    "k_values = np.arange(1, 12)\n",
    "\n",
    "\n",
    "# DTC\n",
    "score = []\n",
    "\n",
    "# Plot the test error of decision trees for different values of the depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4tT7RHQBHmV"
   },
   "source": [
    "We check the train/test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "AWfJlIEOA95b",
    "outputId": "34136186-43a8-4505-c8e8-efadc1a43fe7"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "\n",
    "depths = np.arange(1, 30)\n",
    "criteria = [\"gini\", \"entropy\"]\n",
    "\n",
    "train_err = []\n",
    "test_err = []\n",
    "for criterion in criteria:\n",
    "    train_err.append([])  # New criteria => new list\n",
    "    test_err.append([])\n",
    "    for depth in depths:\n",
    "        clf = DecisionTreeClassifier(criterion=criterion, max_depth=depth)\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_err[-1].append(1-clf.score(X_train, y_train))\n",
    "        test_err[-1].append(1-clf.score(X_test, y_test))\n",
    "        \n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = \"bgrc\"\n",
    "for ic, criterion in enumerate(criteria):\n",
    "    plt.plot(depths, train_err[ic], color=colors[ic], linestyle=':',\n",
    "             label=criterion+\" (train)\")\n",
    "    plt.plot(depths, test_err[ic], color=colors[ic], linestyle='-',\n",
    "             label=criterion+\" (test)\")\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KA8nTZ6dA-GW"
   },
   "source": [
    "Well, depth = 7 seems a good choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAdAg52PclrL"
   },
   "source": [
    "We check also with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WJddEdGd_hqC",
    "outputId": "417544b4-5c77-40bc-9cca-0ff0c0fda98c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Answer\n",
    "score = []\n",
    "for k in k_values:\n",
    "    clf = DecisionTreeClassifier(max_depth=k)\n",
    "    score.append(cross_val_score(clf, X_train, y_train).mean())\n",
    "    \n",
    "ik = np.argmax(score)\n",
    "n_neighbors = k_values[ik]\n",
    "clf = DecisionTreeClassifier(max_depth=n_neighbors)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Depth:\", n_neighbors)\n",
    "print(\"Crossval score:\", score[ik])\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "R_ooaQIVAAdv",
    "outputId": "f89b01fa-2092-473d-800e-f4e7d70cb588"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_pred, y, classes=None, normalize=False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    title='Confusion matrix'\n",
    "    cmap=plt.cm.Blues\n",
    "    \n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    if classes is None:\n",
    "        classes = np.unique(y)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = 'Normalized confusion matrix'\n",
    "    else:\n",
    "        title = 'Unnormalized confusion matrix'\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "plot_confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftV25Lp5_0k2"
   },
   "source": [
    "And we plot de DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hf3Ucqk5cpSp",
    "outputId": "1a736d82-0dde-410b-a6ec-b9f9c0fb7665"
   },
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "clf = DecisionTreeClassifier(max_depth=7)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "#tree.plot_tree(clf.fit(X_train, y_train))\n",
    "\n",
    "!pip install graphviz\n",
    "\n",
    "import graphviz \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, leaves_parallel = True, impurity=False, feature_names = X_raw.columns, class_names=['less 50K', 'greater 50K'], filled=True, rounded=True, special_characters=True, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23Vax5k9_4Jk"
   },
   "source": [
    "Look at the last decisions to see \"unfairness\" (people having same background but won't be paid similarly due to their last feature ('race', 'gender'...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1wkegJmBqyN"
   },
   "source": [
    "## Bagging\n",
    "Bagging is a portmanteau word for *bootstrap aggregating*.\n",
    "The paradigm of bagging is to train independently several base classifiers $(g_1, \\dots, g_T)$, with $g_t \\colon \\mathbb R^d \\to \\{\\pm 1\\}$, and to build a new classifier by averaging the predictions of the base classifiers:\n",
    "$$\n",
    "    g_n^T(x) = \\operatorname{sign} \\left( \\frac{1}{T} \\sum_{t=1}^T g_t(x) \\right).\n",
    "$$\n",
    "Doing so, the variance of the prediction is reduced and so it is for the global error.\n",
    "The requirements for such a result are:\n",
    "- base classifiers should be more accurate than chance;\n",
    "- base classifiers should be estimated independently from each other.\n",
    "\n",
    "In practice, base classifiers are trained *quasi-independently* by bootstrapping the training set.\n",
    "\n",
    "Bagging is also valid for multiclass problems: for $C$ classes, the prediction is:\n",
    "$$\n",
    "    g_n^T(x)\n",
    "    = \\operatorname{arg\\,max}_{1 \\le j \\le C} \\frac{1}{T} \\sum_{t=1}^T g_t(x) \\mathbb 1_{g_t(x)=j}\n",
    "    = \\operatorname{arg\\,max}_{1 \\le j \\le C} \\operatorname{card} \\left( \\left\\{ t \\in [T] : g_t(x) \\mathbb 1_{g_t(x)=j} \\right\\} \\right),\n",
    "$$\n",
    "where $g_t \\colon \\mathbb R^d \\to [C]$, which corresponds to the majority vote since base classifiers are equally weighted.\n",
    "\n",
    "Finally, one may also bag regressors $g_t \\colon \\mathbb R^d \\to \\mathbb R$ by a simple averaging:\n",
    "$$\n",
    "    g_n^T(x) = \\frac{1}{T} \\sum_{t=1}^T g_t(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXuOTdOHBugq"
   },
   "source": [
    "Assume that we are provided with a sequence of independent classifiers $(g_1, \\dots, g_T)$, with $g_t \\colon \\mathbb R^d \\to \\{\\pm 1\\}$, such that classifiers are equally good: there exists $p>0.5$ such that $\\mathbb P(g_t(X)=Y) = p$ for all $t \\in [T]$.\n",
    "We now consider the bagged classifier\n",
    "$$\n",
    "    g_n^T(x) = \\operatorname{sign} \\left( \\frac{1}{T} \\sum_{t=1}^T g_t(x) \\right).\n",
    "$$\n",
    "What is the distribution of the random variable $\\sum_{t=1}^T \\mathbb 1_{g_t(X)=Y}$?\n",
    "Plot its probability mass function for $T=9$ and $p=0.7$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5rnZu7TBy5B"
   },
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "0jBKsYBWu5Md",
    "outputId": "ca2a6f15-5482-4e2c-e488-4d205df23cb2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U0EZFMorB9XW"
   },
   "source": [
    "For these particular values of $T$ and $p$, compute numerically $\\mathbb P(g_n^T(X)=Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qGzplIQWCB-d"
   },
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2mZT6PdBCBuE",
    "outputId": "01fbc80b-a1bd-4bee-f807-ae8f75b334ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRY-UmfICHjm"
   },
   "source": [
    "Plot the probability of success (or accuracy) with respect to the number of base classifiers for $p \\in [0.55, 0.65, \\dots, 0.95]$.\n",
    "Be careful, the formula used previously is only valid for odd numbers of base classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "colab_type": "code",
    "id": "6dI6_BH5B_LF",
    "outputId": "37d818dd-f908-47f4-842e-5f93d648b43c"
   },
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uhx4c6ITCNAa"
   },
   "source": [
    "Complete the following script to implement bagging with regression trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQUXptH6CNZw"
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "class BaggingTree(object):\n",
    "    def __init__(self, n_estimators=10, max_depth=1, max_samples=1.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - n_estimators: number of estimators\n",
    "        - max_depth: maximal depth of the regressor tree\n",
    "        - max_samples: ratio of samples to use for learning base regressors.\n",
    "            - If max_samples=1.0: use bootstrap.\n",
    "            - If max_samples<1.0: use random sampling and extract max_samples x n points\n",
    "            (where n is the total numer of points).\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Make X a 2d-array\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, np.newaxis]\n",
    "            \n",
    "        n = X.shape[0]  # Sample size\n",
    "        m = int(self.max_samples * n)  # Number of points for random sampling\n",
    "        \n",
    "        self.estimators_ = []\n",
    "        for t in range(self.n_estimators):\n",
    "            if self.max_samples==1:\n",
    "                idx = np.random.randint(0, n, n)  # Bootstrap\n",
    "            else:\n",
    "                idx = np.random.permutation(n)[:m]  # Random sampling\n",
    "            self.estimators_.append(DecisionTreeRegressor(max_depth=self.max_depth))\n",
    "            self.estimators_[-1].fit(X[idx], y[idx])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make X a 2d-array\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, np.newaxis]\n",
    "        #COMPLETE HERE\n",
    "    \n",
    "    def error(self, X, y):\n",
    "        # Make X a 2d-array\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, np.newaxis]\n",
    "        return np.sum((y - self.predict(X))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0E8Vh4gREUZy"
   },
   "source": [
    "Apply bagging to the following regression dataset and plot (on the same figure), the training data and the prediction for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "Lp8m7Ne6EUDx",
    "outputId": "b150d7a9-8ee7-443b-d8d6-fd9ae5e47d47"
   },
   "outputs": [],
   "source": [
    "# Regression dataset\n",
    "n = 100\n",
    "X_train = np.sort(5 * np.random.rand(n))\n",
    "y_train = np.sin(X_train)\n",
    "y_train[::5] += 1 * (0.5 - np.random.rand(n//5))\n",
    "\n",
    "X_test = np.arange(0, 5, step=1e-2)\n",
    "y_test = np.sin(X_test)\n",
    "\n",
    "# Make 2d-arrays\n",
    "X_train = X_train[:, np.newaxis]\n",
    "X_test = X_test[:, np.newaxis]\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "\n",
    "# Answer\n",
    "reg = BaggingTree()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_test, y_pred, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "So59iNiDETrR"
   },
   "source": [
    "Analyze the behavior of the prediction curve and of the test error with respect to the maximal depth of decision trees and to the number of base regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wyK2TF5mESyA",
    "outputId": "a64de0b8-d262-40bf-9052-f2d2e5fd48aa"
   },
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCmd4XaKCxcG"
   },
   "source": [
    "We consider decision trees with *max\\_depth = 5*.\n",
    "Plot two curves (one with bootstrap and one with 25%-subsampling) showing the test error with respect to the number of base regressors.\n",
    "What can we conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "BVoYnHzUEaKS",
    "outputId": "54f92f2e-a71d-44f2-dab6-acfab675ae28"
   },
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZR-5b4IEe2T"
   },
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3C-MWkCrEfaa"
   },
   "source": [
    "Random forests are bagged trees: for binary classification, a random forest is\n",
    "$$\n",
    "    g_n^T(x) = \\operatorname{sign} \\left( \\frac{1}{T} \\sum_{t=1}^T g_t(x) \\right),\n",
    "$$\n",
    "where the base classifiers $(g_1, \\dots, g_T)$, with $g_t \\colon \\mathbb R^d \\to \\{\\pm 1\\}$, are learned quasi-independently by bootstrap.\n",
    "\n",
    "However, in order to enforce the independent learning, each decision tree $g_t$ owns an additional randomization step in its learning procedure:\n",
    "\n",
    "1. at each cell, select a subset of features at random;\n",
    "1. find the best pair (feature, threshold) for splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wU3yKzGCEhIU"
   },
   "source": [
    "The following script loads and preprocesses the [diabetes dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html).\n",
    "Explain each step and indicate its purpose.\n",
    "\n",
    "Is it useful for decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEbSz3GjElgT"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler() # not necessary here \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train.reshape(-1, 1))[:, 0]\n",
    "y_test = scaler.transform(y_test.reshape(-1, 1))[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yw0A9RsMEmDA"
   },
   "source": [
    "On the [diabetes dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html), compare scikit-learn [bagging](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) (with bootstrap and 25%-subsampling) and [random forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n",
    "For this purpose, consider *max\\_depth = 5* and plot three curves showing the regression score for the test set with respect to the number of base regressors.\n",
    "What can we conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "cZ-50gQmEom7",
    "outputId": "93943345-6e57-4719-91dd-e2a173ea05ba"
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fairness.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
