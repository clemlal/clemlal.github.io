{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVlR9i5yAtuClJJIZQ3AyL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine Learning Notebook: Linear and Ridge Regression, Dimensionality Reduction and Hyperparameter Tuning\n"],"metadata":{"id":"10aHNItMeis4"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"p0ihwLsRdnjr","executionInfo":{"status":"ok","timestamp":1726154839434,"user_tz":-120,"elapsed":6743,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"outputs":[],"source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LinearRegression, Ridge\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.random_projection import GaussianRandomProjection\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import KFold\n","\n","# Set plot style\n","sns.set(style='whitegrid')"]},{"cell_type":"markdown","source":["# 1. Introduction\n","\n","In this notebook, we will explore Linear Regression and Ridge Regression, focusing on hyperparameter tuning\n","and dimensionality reduction techniques such as PCA (Principal Component Analysis) and Random Projections.\n","Dimensionality reduction can be helpful when dealing with high-dimensional data, both for efficiency and reducing noise.\n"],"metadata":{"id":"p7FKTiOAe0TJ"}},{"cell_type":"code","source":["# Generate a high-dimensional dataset\n","X, y = make_regression(n_samples=1000, n_features=999, n_informative=1, noise=20.)\n","\n","# Add a column of ones to X for the intercept\n","X = np.c_[np.ones((X.shape[0], 1)), X]\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"],"metadata":{"id":"gvjV0iSOet3I","executionInfo":{"status":"ok","timestamp":1726154841945,"user_tz":-120,"elapsed":644,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Linear Regression"],"metadata":{"id":"p7M_WEHgi6bU"}},{"cell_type":"markdown","source":["Linear regression is a method used to model the relationship between a dependent variable $y$ and one or more independent variables $X$. The model assumes a linear relationship between the inputs and the output:\n","\n","$$\n","y = X\\beta + \\epsilon\n","$$\n","\n","Where:\n","\n","- $X$ is the matrix of input features (with each row representing a data point).\n","    \n","- $\\beta$ are the coefficients (parameters) we want to estimate.\n","    \n","- $\\epsilon$ is the error term (assumed to be normally distributed).\n","\n","The goal of linear regression is to find the parameters $\\beta$ that minimize the sum of squared errors (SSE):\n","\n","$$\n","\\text{SSE} = \\sum_{i=1}^n (y_i - X_i \\beta)^2\n","$$\n","\n","This minimization problem is solved by computing the ordinary least squares (OLS) estimate:\n","\n","$$\n","\\hat{\\beta} = (X^T X)^{-1} X^T y\n","$$\n","\n","This equation gives the optimal parameters $\\beta$ that minimize the prediction error."],"metadata":{"id":"YqXhfYck-Rpp"}},{"cell_type":"markdown","source":["**Question 1** : Implement the linear regression using the class sklearn.linear_model.LinearRegression."],"metadata":{"id":"1yDUnmHsc3Bo"}},{"cell_type":"code","source":[],"metadata":{"id":"lafapyTOGSiG","executionInfo":{"status":"ok","timestamp":1726154871543,"user_tz":-120,"elapsed":289,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["**Question 2** : Implement the linear regression by hand (using Numpy functions only)."],"metadata":{"id":"puieXmLtdYdd"}},{"cell_type":"code","source":[],"metadata":{"id":"SOotFm-xTctO","executionInfo":{"status":"ok","timestamp":1726154887752,"user_tz":-120,"elapsed":312,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Ridge Regression\n","\n","Ridge Regression is a regularized version of linear regression that addresses multicollinearity and prevents overfitting by adding a penalty term to the cost function. It modifies the linear regression objective by introducing an L2 regularization term to penalize large coefficients:\n","\n","$$\n","\\text{Ridge Cost Function} = \\sum_{i=1}^n (y_i - X_i \\beta)^2 + \\alpha \\sum_{j=1}^p \\beta_j^2\n","$$\n","\n","Where:\n","\n","- $\\alpha$ is the regularization parameter controlling the penalty strength.\n","    \n","- $\\beta_j$ are the regression coefficients.\n","\n","The second term $\\alpha \\sum_{j=1}^p \\beta_j^2$ discourages large values of $\\beta$, which helps prevent overfitting in high-dimensional or multicollinear datasets. The solution to the ridge regression is given by:\n","\n","$$\n","\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\alpha I)^{-1} X^T y\n","$$\n","\n","Where $I$ is the identity matrix. The addition of $\\alpha I$ ensures that the matrix is invertible, even in cases of multicollinearity."],"metadata":{"id":"AcTGP1YPANPf"}},{"cell_type":"markdown","source":["**Question 1** : Implement the Ridge regression using the function class sklearn.linear_model.Ridge."],"metadata":{"id":"AxMePwXZbJuW"}},{"cell_type":"code","source":[],"metadata":{"id":"ePjFJ6OoGT1T","executionInfo":{"status":"ok","timestamp":1726154901253,"user_tz":-120,"elapsed":295,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**Question 2** : Implement the Ridge regression by hand (using Numpy functions only)."],"metadata":{"id":"OUfGL6y-biNg"}},{"cell_type":"code","source":[],"metadata":{"id":"O5UViPDFUvf8","executionInfo":{"status":"ok","timestamp":1726154917654,"user_tz":-120,"elapsed":531,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**Question 3** : Plot the train and test errors of the model as a function of $\\alpha$."],"metadata":{"id":"2-AwF9rUbw9L"}},{"cell_type":"code","source":[],"metadata":{"id":"ThgMDXGMXjT9","executionInfo":{"status":"ok","timestamp":1726154939293,"user_tz":-120,"elapsed":285,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameter Tuning 1 : Optimization over the valitation set\n","\n","Normal Validation (or train/validation split) is a common approach for evaluating a machine learning model. The dataset is split into two sets:\n","\n","- Training set: Used to train the model.\n","\n","- Validation set: Used to evaluate the model's performance on unseen data.\n","\n","Mathematically, this can be represented as:\n","\n","$$\n","X_{\\text{train}}, y_{\\text{train}} \\quad \\text{and} \\quad X_{\\text{val}}, y_{\\text{val}}\n","$$\n","\n","The model is trained on $(X_{\\text{train}}, y_{\\text{train}})$ and evaluated on $(X_{\\text{val}}, y_{\\text{val}})$. This process helps detect overfitting because the model is tested on data that it hasnâ€™t seen during training.\n","\n","The performance metric (e.g., mean squared error) is calculated on the validation set:\n","\n","$$\n","\\text{MSE}_{\\text{val}} = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} (y_{\\text{val}_i} - \\hat{y}_{\\text{val}_i})^2\n","$$\n","\n","This score is then used in hyperparameter tuning and model selection."],"metadata":{"id":"1laKX-ohGpoK"}},{"cell_type":"markdown","source":["**Question** : Split the train set in a smaller train set and a validation set and then tune the Ridge parametter on."],"metadata":{"id":"tPgpsg1SsTdu"}},{"cell_type":"code","source":[],"metadata":{"id":"GrVvUFifQJrA","executionInfo":{"status":"ok","timestamp":1726154983897,"user_tz":-120,"elapsed":307,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Dimensionality Reduction 1 : Random Projections\n","\n","Random Projections is a technique used to reduce the dimensionality of data by projecting it onto a randomly chosen subspace of lower dimensions. The idea is based on the Johnson-Lindenstrauss Lemma, which states that a small set of points in a high-dimensional space can be embedded into a lower-dimensional space while approximately preserving pairwise distances.\n","\n","The projection is done using a random matrix $R$ of size $d \\times k$, where $k$ is the reduced dimensionality. The original data $X \\in \\mathbb{R}^{n \\times d}$ is projected onto the lower-dimensional space as follows:\n","\n","$$\n","X_{\\text{RP}} = X R\n","$$\n","\n","Where:\n","\n","- $X_{\\text{RP}} \\in \\mathbb{R}^{n \\times k}$ is the data in the reduced space.\n","    \n","- The entries of RR are typically drawn from a Gaussian distribution or a sparse distribution.\n","\n","Random projections are computationally efficient and often work well in practice, especially in high-dimensional spaces."],"metadata":{"id":"nMzpiMuZFUzq"}},{"cell_type":"markdown","source":["**Question :** Implement random projections (you can use GaussianRandomProjection from sklearn.random_projections) and compare the performance a linear regression to the version without dimensionality reduction."],"metadata":{"id":"ZXvFzk4rwAk4"}},{"cell_type":"code","source":[],"metadata":{"id":"9w5eXYV4FTJM","executionInfo":{"status":"ok","timestamp":1726154999104,"user_tz":-120,"elapsed":284,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameter Tuning 2 : Cross-Validation\n","\n","Cross-Validation is a technique used to ensure that the model generalizes well to unseen data (more complex than regular validation). The most common form is $k$-fold cross-validation, where the dataset is split into $k$ subsets (folds). The model is trained on $k-1$ folds and evaluated on the remaining fold. This process is repeated $k$ times, with each fold serving as the validation set once.\n","\n","Mathematically, for each fold $i$:\n","\n","- Train the model on $k-1$ folds\n","\n","- Validate the model on the $i$-th fold\n","\n","The performance metric (e.g., MSE) is computed for each fold, and the average score (other statistics such as the median, percentiles, ... may be used alternatively) is calculated:\n","\n","$$\n","\\text{MSE}_{\\text{cv}} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{MSE}_{\\text{val}_i}\n","$$\n","\n","Cross-validation helps in reducing the variability of the validation scores and ensures the model is tested on multiple subsets of data, leading to more robust model selection.\n"],"metadata":{"id":"RPb7TFSeQLsi"}},{"cell_type":"markdown","source":["**Question :** Implement cross-validation using KFold from sklearn.model_selection and use it to determine a good projection dimension with random projections."],"metadata":{"id":"TJYKWfOXxV5g"}},{"cell_type":"code","source":[],"metadata":{"id":"505s-2mfRNoa","executionInfo":{"status":"ok","timestamp":1726155029099,"user_tz":-120,"elapsed":308,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Dimensionality Reduction 2 : Principal Component Analysis (PCA)\n","\n","PCA is a dimensionality reduction technique that transforms the original feature space into a new space of principal components. These components are linear combinations of the original features, ordered by the amount of variance they capture.\n","\n","Mathematically, PCA involves the following steps:\n","\n","- Center the Data: Subtract the mean from each feature.\n","    \n","- Covariance Matrix: Compute the covariance matrix $\\Sigma$ of the centered data:\n","\n","$$\n","\\Sigma = \\frac{1}{n} X^T X\n","$$\n","\n","- Eigenvalues and Eigenvectors: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors correspond to the directions of the principal components, and the eigenvalues represent the variance explained by each component.\n","\n","- Projection: Project the original data XX onto the new space defined by the eigenvectors:\n","\n","$$\n","X_{\\text{PCA}} = X W\n","$$\n","\n","Where $W$ is the matrix of eigenvectors corresponding to the largest eigenvalues.\n","\n","The key idea behind PCA is that the first few principal components capture most of the variance, so we can reduce dimensionality by retaining only the top components."],"metadata":{"id":"DKVnmjm3EVbu"}},{"cell_type":"markdown","source":["**Question :** Implement PCA (you can use PCA from sklearn.decomposition) and compare the performance a linear regression to the version without dimensionality reduction."],"metadata":{"id":"Vwdi5BQRyX7t"}},{"cell_type":"code","source":[],"metadata":{"id":"N6l2NOfMi1Qu","executionInfo":{"status":"ok","timestamp":1726155033289,"user_tz":-120,"elapsed":304,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yfE5M6O8yN7z","executionInfo":{"status":"ok","timestamp":1726155018003,"user_tz":-120,"elapsed":301,"user":{"displayName":"C L (RandomUser10010)","userId":"09860949984960822135"}}},"execution_count":11,"outputs":[]}]}