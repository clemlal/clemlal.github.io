<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematics of Machine Learning M2 MAPI3 (2024-2025)</title>
    <link rel="stylesheet" href="../../css/styles.css">
</head>
<body>
    <div class="container">
        <main class="main">
            <div id="toptitle">
                <h1>Mathematics of Machine Learning M2 MAPI3 (Paul Sabatier, 2024-2025)</h1>
                <strong>Lecturer:</strong> <a href="../../index.html" target="_blank" rel="noopener noreferrer">Clément Lalanne</a>
            </div>
            <div id="content">
                <h2>Overview</h2>
                <p>This class aims to introduce the main theoretical foundations of modern machine learning, with a focus on supervised learning. Basic knowledge of linear algebra and probability theory (including measure theory) is required. For the last lectures, basic knowledge of functional analysis is recommended.</p>
            </div>

            
            <h2>Evaluation</h2>
            <p> <strong>IMPORTANT PLEASE READ</strong> I have been informed that the evaluation methods are determined by the university and cannot be altered. I am deeply sorry about the missleading early information that I gave you. Below are the updated evaluation modalities: <br>

                For all students, the final grade will consist of an 80% weight from a 2-hour final exam and 20% from project work. For part-time working students (étudiants en alternance), the project grade will be based on the TP2, which I will evaluate (replacing the previous homework assignment). For other students, the project grade will be split equally: 50% from TP2 and 50% from the actual <a href="../2024_2025_MAPI3_Machine_Learning_Projects/index.html" target="_blank" rel="noopener noreferrer">projects</a>.</p><br>
                <p><strong>The TP2 (which is due for the evaluation) is out.</strong></p>

                <p><strong>Final Exam : 21/02/2025 (13h30 -> 15h30 (+ 40 minutes for people with extra time)).</strong> Here are <a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/sites/default/files/exoML_M1ENSL.pdf" target="_blank" rel="noopener noreferrer">exercices on which you can train on</a>.</p>

                <p><a href="ExamMAPI320242025.pdf" target="_blank" rel="noopener noreferrer">Final Exam 2024 2025</a> (<a href="CorrectionExamMAPI320242025.pdf" target="_blank" rel="noopener noreferrer">solution</a>).</p>
            
            

            <h3>Survival Kit for the Exam</h3>
            <p>For the exam, you should be familiar with the following concepts and techniques:</p>
            <ul>
                <li>Decomposing risk as an estimation error (considering the uniform worst-case error over the predictor set), approximation error, and potential other error sources (e.g., optimization errors). You should be able to recognize the different bias and variance terms in this decomposition.</li>
                <li>Basic linear algebra (vectors, linear maps, rank, matrices) and bilinear algebra (eigenvectors, eigenvalues, spectral theorem, basic matrix decompositions).</li>
                <li>Multivariable calculus (Gradients, Hessians, Taylor expansions, higher-order derivative tensors), Convexity, First and Second Order optimality conditions, KKT conditions.</li>
                <li>Basic probability theory in general probability spaces, conditional laws, conditional expectations, and independence.</li>
                <li>Common inequalities: Triangle inequality, convexity, AM-GM inequality, Cauchy-Schwarz, Jensen, Hölder, Minkowski.</li>
                <li>Concentration inequalities: Markov, Bienaymé-Tchebychev and McDarmid's inequality.</li>
                <li>Rademacher complexity: Definition, upper bounding the sup deviations (each side by two times the Rademacher complexity), Lipschitz contraction principle, and typical use cases.</li>
                <li>Kernel methods: Aronszajn's theorem (equivalence between positive kernels and Hilbert space dot products), representer theorem, kernel trick, operations on kernels, and Bochner's theorem.</li>
                <li>Optimization: general optimization priciples and usual regularity assumptions.</li>
                <li><em>More to be continued...</em></li>
            </ul>

            
            <h2>Lectures</h2>
            <ul>
                <li><strong>Lecture 1</strong> (02/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture1Introduction.pdf" target="_blank" rel="noopener noreferrer">Introduction to Supervised Learning</a>.</li>
                <li><strong>Lecture 2</strong> (04/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture2RegressionsLinearRidge.pdf" target="_blank" rel="noopener noreferrer">Linear Regression and Ridge Regression</a>.</li>
                <li><strong>Lecture 3</strong> (09/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture3ERM1.pdf" target="_blank" rel="noopener noreferrer">Empirical Risk Minimization 1</a>.</li>
                <li><strong>Lecture 4</strong> (11/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture4ERM2.pdf" target="_blank" rel="noopener noreferrer">Empirical Risk Minimization 2</a>.</li>
                <li><strong>Lecture 5</strong> (16/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture5Kernels1.pdf" target="_blank" rel="noopener noreferrer">Kernel Methods 1</a> (<a href="https://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/notes/aronszajn.pdf" target="_blank" rel="noopener noreferrer">Jean-Philippe Vert's proof of Aronszajn's theorem</a>).</li>
                <li><strong>Lecture 6</strong> (18/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture6Kernels2.pdf" target="_blank" rel="noopener noreferrer">Kernel Methods 2</a>.</li>
                <li><strong>Lecture 7</strong> (06/01/2025 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture7Optimization.pdf" target="_blank" rel="noopener noreferrer">Optimization for Machine Learning</a>.</li>
                <li><strong>Lecture 8</strong> (08/01/2025 - 15:45-17:45): <a href="MachineLearningMAPI3Lecture8NeuralNetworks.pdf" target="_blank" rel="noopener noreferrer">Neural Networks</a>.</li>
                <li><strong>Lecture 9</strong> (13/01/2025 - 13:30-15:30): <strong>Questions / Answers before the exam</strong> : To maximize efficiency, prepare your questions before the class !</li>
            </ul>

            <h2>TDs / TPs</h2>
            <ul>
                <li><strong>TD/TP 1</strong> Group 1 (07/10/2024 - 13:30-15:30) - Group 2 (07/10/2024 - 15:45-17:45): <a href="TP1.ipynb" target="_blank" rel="noopener noreferrer">Linear Regression, Ridge Regression, (Cross-)Validation, and Dimensionality Reduction</a> (<a href="TP1_correction.ipynb" target="_blank" rel="noopener noreferrer">solution</a>).</li>
                <li><strong>TD/TP 2</strong> Group 1 (14/10/2024 - 13:30-15:30) - Group 2 (14/10/2024 - 15:45-17:45): <a href="TP2.ipynb" target="_blank" rel="noopener noreferrer">Kernel methods</a> (The correction will be sent by email after the deadline) <strong>Due : 13-12-2024 23h59 by email @ clement.lalanne@univ-tlse3.fr with object [TP2 MAPI3 Firstname LASTNAME]. Complementary info : I expect one submission (i.e. one email) per student, and the theoretical question may be aswered on scanned handwriting (you do not have to write the answers in Latex). One email may contain a Python notebook and a pdf.</strong>.</li>
                <li><strong>TD/TP 3</strong> Group 1 (04/11/2024 - 13:30-15:30) - Group 2 (04/11/2024 - 15:45-17:45): <a href="TP3.ipynb" target="_blank" rel="noopener noreferrer">Neural Networks and Optimization</a> (<a href="TP3_correction.ipynb" target="_blank" rel="noopener noreferrer">solution</a>) original material by <a href="https://perso.ens-lyon.fr/elisa.riccietti/" target="_blank" rel="noopener noreferrer">Elisa Riccietti</a>.</li>
            </ul>

            <h2>References and External Resources</h2>

            <h3>Machine Learning and Learning Theory</h3>
            <ul>
                <li><a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target="_blank" rel="noopener noreferrer">Learning Theory from First Principles</a> by <a href="https://www.di.ens.fr/~fbach/" target="_blank" rel="noopener noreferrer">Francis Bach</a>, 2024: <strong>Main reference for the course.</strong></li>
                <li><a href="https://arxiv.org/abs/2404.17625" target="_blank" rel="noopener noreferrer">Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land</a> by Simone Scardapane, 2024: A very accessible hands on presentation of modern machine learning methods.</li> 
                <li><a href="https://cs.nyu.edu/~mohri/mlbook/" target="_blank" rel="noopener noreferrer">Foundations of Machine Learning</a> by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, 2018.</li>
                <li><a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/index.html" target="_blank" rel="noopener noreferrer">Aurélien Garivier</a>'s <a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/index2baa.html?q=node/214" target="_blank" rel="noopener noreferrer">ML M2 class</a> at ENS Lyon: Complementary topics to those presented in this course.</li>
            </ul>

            <h3>Optimization for Machine Learning</h3>
            <ul>
                <li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener noreferrer">Convex Optimization</a> by Stephen Boyd and Lieven Vandenberghe, Cambridge University Press, 2012.</li>
                <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611974997" target="_blank" rel="noopener noreferrer">First-Order Optimization Methods</a> by Amir Beck, 2017.</li>
                <li><a href="https://arxiv.org/pdf/2101.09545" target="_blank" rel="noopener noreferrer">Acceleration Methods</a> by Alexandre d'Aspremont, Damien Scieur and Adrien Taylor, 2024.</li>
            </ul>

            <h3>Measure and Probability Theory</h3>
            <ul>
                <li><a href="https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/IPPA2.pdf" target="_blank" rel="noopener noreferrer">Intégration, Probabilités et Processus Aléatoires</a> by Jean-François Le Gall, 2006: An excellent introduction to measure theory with elements of stochastic processes and conditional expectation.</li>
                <li>Probabilités 2 by Jean-Yves Ouvrard, 2009: A detailed treatment of advanced topics in probability theory.</li>
                <li>Concentration Inequalities: A Nonasymptotic Theory of Independence by Stéphane Boucheron, Gábor Lugosi, and Pascal Massart, 2013.</li>
            </ul>

            <h3>Analysis</h3>
            <ul>
                <li><a href="http://www.cmap.polytechnique.fr/~massot/MAP431_web/MAP431_Histoire_Massot/Livres_Bony_Lutzen/Jean-Michel%20Bony-Cours%20d%27analyse.%20The%CC%81orie%20des%20distributions%20et%20analyse%20de%20Fourier-Ellipses(2001).pdf" target="_blank" rel="noopener noreferrer">Cours d'analyse</a> by Jean-Michel Bony, 2001.</li>
            </ul>

        </main>
    </div>
</body>
</html>
