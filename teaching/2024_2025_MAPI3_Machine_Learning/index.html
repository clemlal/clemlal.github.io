<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematics of Machine Learning M2 MAPI3 (Paul Sabatier, 2024-2025)</title>
    <link rel="stylesheet" href="../../css/styles.css">
</head>
<body>
    <div class="container">
        <main class="main">
            <div id="toptitle">
                <h1>Mathematics of Machine Learning M2 MAPI3 (Paul Sabatier, 2024-2025)</h1>
                <strong>Lecturer:</strong> <a href="../../index.html" target="_blank" rel="noopener noreferrer">Clément Lalanne</a>
            </div>
            <div id="content">
                <h2>Overview</h2>
                <p>This class aims to introduce the main theoretical foundations of modern machine learning, with a focus on supervised learning. Basic knowledge of linear algebra and probability theory (including measure theory) is required. For the last lectures, basic knowledge of functional analysis is recommended.</p>
            </div>

            
            <h2>Evaluation</h2>
            <p> <strong>IMPORTANT PLEASE READ</strong> I have been informed that the evaluation methods are determined by the university and cannot be altered. I am deeply sorry about the missleading early information that I gave you. Below are the updated evaluation modalities: <br>

                For all students, the final grade will consist of an 80% weight from a 2-hour final exam and 20% from project work. For part-time working students (étudiants en alternance), the project grade will be based on the TP2, which I will evaluate (replacing the previous homework assignment). For other students, the project grade will be split equally: 50% from TP2 and 50% from the actual projects.</p>
            
            

            <h3>Survival Kit for the Exam</h3>
            <p>For the exam, you should be familiar with the following concepts and techniques:</p>
            <ul>
                <li>Decomposing risk as an estimation error (considering the uniform worst-case error over the predictor set), approximation error, and potential other error sources (e.g., optimization errors). You should be able to recognize the different bias and variance terms in this decomposition.</li>
                <li>Basic linear algebra (vectors, linear maps, rank, matrices) and bilinear algebra (eigenvectors, eigenvalues, spectral theorem, basic matrix decompositions).</li>
                <li>Multivariable calculus (Gradients, Hessians, Taylor expansions, higher-order derivative tensors), Convexity, First and Second Order optimality conditions, KKT conditions.</li>
                <li>Basic probability theory in general probability spaces, conditional laws, conditional expectations, and independence.</li>
                <li>Common inequalities: Triangle inequality, convexity, AM-GM inequality, Cauchy-Schwarz, Jensen, Hölder, Minkowski.</li>
                <li>Concentration inequalities: Markov, Bienaymé-Tchebychev and McDarmid's inequality.</li>
                <li>Rademacher complexity: Definition, upper bounding the sup deviations (each side by two times the Rademacher complexity), Lipschitz contraction principle, and typical use cases.</li>
                <li>Kernel methods: Aronszajn's theorem (equivalence between positive kernels and Hilbert space dot products), representer theorem, kernel trick, operations on kernels, and Bochner's theorem (positive kernels are Fourier transforms of symmetric positive Borel measures).</li>
                <li><em>More to be continued...</em></li>
            </ul>

            
            <h2>Lectures</h2>
            <ul>
                <li><strong>Lecture 1</strong> (02/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture1Introduction.pdf" target="_blank" rel="noopener noreferrer">Introduction to Supervised Learning</a>.</li>
                <li><strong>Lecture 2</strong> (04/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture2RegressionsLinearRidge.pdf" target="_blank" rel="noopener noreferrer">Linear Regression and Ridge Regression</a>.</li>
                <li><strong>Lecture 3</strong> (09/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture3ERM1.pdf" target="_blank" rel="noopener noreferrer">Empirical Risk Minimization 1</a>.</li>
                <li><strong>Lecture 4</strong> (11/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture4ERM2.pdf" target="_blank" rel="noopener noreferrer">Empirical Risk Minimization 2</a>.</li>
                <li><strong>Lecture 5</strong> (16/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture5Kernels1.pdf" target="_blank" rel="noopener noreferrer">Kernel Methods 1</a> (<a href="https://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/notes/aronszajn.pdf" target="_blank" rel="noopener noreferrer">Jean-Philippe Vert's proof of Aronszajn's theorem</a>).</li>
                <li><strong>Lecture 6</strong> (18/09/2024 - 13:30-15:30): <a href="MachineLearningMAPI3Lecture6Kernels2.pdf" target="_blank" rel="noopener noreferrer">Kernel Methods 2</a>.</li>
                <li><strong>Lecture 7</strong> (06/01/2025 - 13:30-15:30): <em>TBD</em>.</li>
                <li><strong>Lecture 8</strong> (08/01/2025 - 15:45-17:45): <em>TBD</em>.</li>
                <li><strong>Lecture 9</strong> (13/01/2025 - 13:30-15:30): <em>TBD</em>.</li>
            </ul>

            <h2>TDs / TPs</h2>
            <ul>
                <li><strong>TD/TP 1</strong> Group 1 (07/10/2024 - 13:30-15:30) - Group 2 (07/10/2024 - 15:45-17:45): <a href="TP1.ipynb" target="_blank" rel="noopener noreferrer">Linear Regression, Ridge Regression, (Cross-)Validation, and Dimensionality Reduction</a> (<a href="TP1_correction.ipynb" target="_blank" rel="noopener noreferrer">solution</a>).</li>
                <li><strong>TD/TP 2</strong> Group 1 (14/10/2024 - 13:30-15:30) - Group 2 (14/10/2024 - 15:45-17:45): <em>TBD</em>.</li>
                <li><strong>TD/TP 3</strong> Group 1 (04/11/2024 - 13:30-15:30) - Group 2 (04/11/2024 - 15:45-17:45): <em>TBD</em>.</li>
            </ul>

            <h2>References and External Resources</h2>

            <h3>Machine Learning and Learning Theory</h3>
            <ul>
                <li><a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target="_blank" rel="noopener noreferrer">Learning Theory from First Principles</a> by <a href="https://www.di.ens.fr/~fbach/" target="_blank" rel="noopener noreferrer">Francis Bach</a>, 2024: <strong>Main reference for the course.</strong></li>
                <li><a href="https://cs.nyu.edu/~mohri/mlbook/" target="_blank" rel="noopener noreferrer">Foundations of Machine Learning</a> by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, 2018.</li>
                <li><a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/index.html" target="_blank" rel="noopener noreferrer">Aurélien Garivier</a>'s <a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/index2baa.html?q=node/214" target="_blank" rel="noopener noreferrer">ML M2 class</a> at ENS Lyon: Complementary topics to those presented in this course.</li>
            </ul>

            <h3>Optimization for Machine Learning</h3>
            <ul>
                <li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener noreferrer">Convex Optimization</a> by Stephen Boyd and Lieven Vandenberghe, Cambridge University Press, 2012.</li>
                <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611974997" target="_blank" rel="noopener noreferrer">First-Order Optimization Methods</a> by Amir Beck, 2017.</li>
            </ul>

            <h3>Measure and Probability Theory</h3>
            <ul>
                <li><a href="https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/IPPA2.pdf" target="_blank" rel="noopener noreferrer">Intégration, Probabilités et Processus Aléatoires</a> by Jean-François Le Gall, 2006: An excellent introduction to measure theory with elements of stochastic processes and conditional expectation.</li>
                <li>Probabilités 2 by Jean-Yves Ouvrard, 2009: A detailed treatment of advanced topics in probability theory.</li>
                <li>Concentration Inequalities: A Nonasymptotic Theory of Independence by Stéphane Boucheron, Gábor Lugosi, and Pascal Massart, 2013.</li>
            </ul>

            <h3>Analysis</h3>
            <ul>
                <li><a href="http://www.cmap.polytechnique.fr/~massot/MAP431_web/MAP431_Histoire_Massot/Livres_Bony_Lutzen/Jean-Michel%20Bony-Cours%20d%27analyse.%20The%CC%81orie%20des%20distributions%20et%20analyse%20de%20Fourier-Ellipses(2001).pdf" target="_blank" rel="noopener noreferrer">Cours d'analyse</a> by Jean-Michel Bony, 2001.</li>
            </ul>

        </main>
    </div>
</body>
</html>
