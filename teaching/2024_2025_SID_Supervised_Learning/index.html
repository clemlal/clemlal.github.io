<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematical Methods for Supervised Learning M1 SID (Paul Sabatier, 2024-2025)</title>
    <link rel="stylesheet" href="../../css/styles.css">
</head>
<body>
    <div class="container">
        <main class="main">
            <div id="toptitle">
                <h1>Mathematical Methods for Supervised Learning M1 SID (2024-2025)</h1>
                <strong>Lecturer:</strong> <a href="../../index.html" target="_blank" rel="noopener noreferrer">Clément Lalanne</a>
            </div>
            <div id="content">
                <h2>Overview</h2>
                <p>This course introduces mathematical methods essential for supervised learning. Topics include linear models, optimization techniques, and advanced methods like kernel and sparse methods. Students are expected to have basic knowledge of linear algebra, calculus, and probability.</p>
            </div>

            <h2>Evaluation</h2>
            <p>The class will be evaluated by a final exam (70%) and a homework (30%)</p>

            <h2>Lectures</h2>
            <ul>
                <li><strong>Lecture 1</strong> (21-01-2025 15h45->17h45): <a href="SupervisedLearningSIDM1_Introduction_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Introduction to Supervised Learning</a>.</li>
                <li><strong>Lecture 2</strong> (24-01-2025 7h45->9h45): <a href="SupervisedLearningSIDM1_LinearModels_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Linear Models</a>.</li>
                <li><strong>Lecture 3</strong> (28-01-2025 7h45->9h45): <a href="SupervisedLearningSIDM1_Optimization_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Elements of Optimization</a>.</li>
                <li><strong>Lecture 4</strong> (30-01-2025 15h45->17h45): <a href="SupervisedLearningSIDM1_ModelSelectionSparseMethods_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Validation, Model Selection and Sparse Methods</a>.</li>
                <li><strong>Lecture 5</strong> (04-02-2025 7h45->9h45): <a href="SupervisedLearningSIDM1_CARTBoosting_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Classification and Regression Trees + Boosting</a>.</li>
                <li><strong>Lecture 6</strong> (07-02-2025 13h30->15h30): <a href="SupervisedLearningSIDM1_KernelMethods_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Kernel Methods</a> (<a href="https://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/notes/aronszajn.pdf" target="_blank" rel="noopener noreferrer">Jean-Philippe Vert's proof of Aronszajn's theorem</a>).</li>
                <li><strong>Lecture 7</strong> (11-02-2025 15h45->17h45):<a href="SupervisedLearningSIDM1_NeuralNetworks_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Neural Networks</a>.</li>
                <li><strong>Lecture 8</strong> (17-02-2025 10h00->12h00): <strong>Questions / Answers before the exam</strong> : To maximize efficiency, prepare your questions before the class !</li>
            </ul>

            <h2>TDs</h2>
            <ul>
                <li><strong>TD 1</strong> (Date TBD): <a href="TD1_SID_M1_App_Sup_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Elements of Probability and Statistics</a> (<a href="Correction_TD1_SIDM1_AppSup.pdf" target="_blank" rel="noopener noreferrer">solution</a>). </li>
                <li><strong>TD 2</strong> (Date TBD): <a href="TD2_SID_M1_App_Sup_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Logistic Regression</a> (<a href="Correction_TD2_SIDM1_AppSup.pdf" target="_blank" rel="noopener noreferrer">solution</a>).</li>
                <li><strong>TD 3</strong> (Date TBD): <a href="TD3_SID_M1_App_Sup_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Trees</a> (<a href="Correction_TD3_SIDM1_AppSup.pdf" target="_blank" rel="noopener noreferrer">solution</a>).</li>
                <li><strong>TD 4</strong> (Date TBD): <a href="TD4_SID_M1_App_Sup_2024_2025.pdf" target="_blank" rel="noopener noreferrer">Kernel Methods</a>.</li>
            </ul>
            Some of the original material was made by <a href="https://www.math.univ-toulouse.fr/~fbachoc/" target="_blank" rel="noopener noreferrer">François Bachoc</a> and by <a href="https://perso.math.univ-toulouse.fr/amazoyer/" target="_blank" rel="noopener noreferrer">Adrien Mazoyer</a>.

            <h2>TPs</h2>
            <ul>
                <li><strong>TP 1</strong> (Date TBD): <a href="TP1.ipynb" target="_blank" rel="noopener noreferrer">Linear Regression, Ridge Regression, and (Cross-)Validation</a>  (<a href="TP1_correction.ipynb" target="_blank" rel="noopener noreferrer">solution</a>)</li>
                <li><strong>TP 2</strong> (Date TBD): <a href="TP2.ipynb" target="_blank" rel="noopener noreferrer">Kernel SVM</a>.</li>
                <li><strong>TP 3</strong> (Date TBD): TBD</li>
            </ul>

            <h2>References and External Resources</h2>

            <h3>Machine Learning and Learning Theory</h3>
            <ul>
                <li><a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target="_blank" rel="noopener noreferrer">Learning Theory from First Principles</a> by <a href="https://www.di.ens.fr/~fbach/" target="_blank" rel="noopener noreferrer">Francis Bach</a>, 2024: <strong>Main reference for the course.</strong></li>
                <li><a href="https://arxiv.org/abs/2404.17625" target="_blank" rel="noopener noreferrer">Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land</a> by Simone Scardapane, 2024: A very accessible hands on presentation of modern machine learning methods.</li> 
                <li><a href="https://cs.nyu.edu/~mohri/mlbook/" target="_blank" rel="noopener noreferrer">Foundations of Machine Learning</a> by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, 2018.</li>
            </ul>

            <h3>Optimization for Machine Learning</h3>
            <ul>
                <li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener noreferrer">Convex Optimization</a> by Stephen Boyd and Lieven Vandenberghe, Cambridge University Press, 2012.</li>
                <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611974997" target="_blank" rel="noopener noreferrer">First-Order Optimization Methods</a> by Amir Beck, 2017.</li>
                <li><a href="https://arxiv.org/pdf/2101.09545" target="_blank" rel="noopener noreferrer">Acceleration Methods</a> by Alexandre d’Aspremont, Damien Scieur and Adrien Taylor, 2024.</li>
            </ul>

            <h3>Measure and Probability Theory</h3>
            <ul>
                <li><a href="https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/IPPA2.pdf" target="_blank" rel="noopener noreferrer">Intégration, Probabilités et Processus Aléatoires</a> by Jean-François Le Gall, 2006: An excellent introduction to measure theory with elements of stochastic processes and conditional expectation.</li>
                <li>Probabilités 2 by Jean-Yves Ouvrard, 2009: A detailed treatment of advanced topics in probability theory.</li>
                <li>Concentration Inequalities: A Nonasymptotic Theory of Independence by Stéphane Boucheron, Gábor Lugosi, and Pascal Massart, 2013.</li>
            </ul>

            <h3>Analysis</h3>
            <ul>
                <li><a href="http://www.cmap.polytechnique.fr/~massot/MAP431_web/MAP431_Histoire_Massot/Livres_Bony_Lutzen/Jean-Michel%20Bony-Cours%20d%27analyse.%20The%CC%81orie%20des%20distributions%20et%20analyse%20de%20Fourier-Ellipses(2001).pdf" target="_blank" rel="noopener noreferrer">Cours d'analyse</a> by Jean-Michel Bony, 2001.</li>
            </ul>

        </main>
    </div>
</body>
</html>
