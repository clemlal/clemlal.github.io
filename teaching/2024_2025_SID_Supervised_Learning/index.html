<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematical Methods for Supervised Learning M1 SID (Paul Sabatier, 2024-2025)</title>
    <link rel="stylesheet" href="../../css/styles.css">
</head>
<body>
    <div class="container">
        <main class="main">
            <div id="toptitle">
                <h1>Mathematical Methods for Supervised Learning M1 SID (Paul Sabatier, 2024-2025)</h1>
                <strong>Lecturer:</strong> <a href="../../index.html" target="_blank" rel="noopener noreferrer">Clément Lalanne</a>
            </div>
            <div id="content">
                <h2>Overview</h2>
                <p>This course introduces mathematical methods essential for supervised learning. Topics include linear models, optimization techniques, and advanced methods like kernel and sparse methods. Students are expected to have basic knowledge of linear algebra, calculus, and probability.</p>
            </div>

            <h2>Evaluation</h2>
            <p>Details about the evaluation will be provided later.</p>

            <h3>Survival Kit for the Exam</h3>
            <p>Details about the exam preparation will be provided later.</p>

            <h2>Lectures</h2>
            <ul>
                <li><strong>Lecture 1</strong> (Date TBD): Introduction to Supervised Learning</li>
                <li><strong>Lecture 2</strong> (Date TBD): Linear Models</li>
                <li><strong>Lecture 3</strong> (Date TBD): Elements of Optimization</li>
                <li><strong>Lecture 4</strong> (Date TBD): Classification and Regression Trees</li>
                <li><strong>Lecture 5</strong> (Date TBD): Kernel Methods 1</li>
                <li><strong>Lecture 6</strong> (Date TBD): Kernel Methods 2</li>
                <li><strong>Lecture 7</strong> (Date TBD): Neural Networks</li>
                <li><strong>Lecture 8</strong> (Date TBD): Sparse Methods</li>
            </ul>

            <h2>TDs</h2>
            <ul>
                <li><strong>TD 1</strong> (Date TBD): Elements of Statistics (Group 1 & 2)</li>
                <li><strong>TD 2</strong> (Date TBD): Linear Models (Group 1 & 2)</li>
                <li><strong>TD 3</strong> (Date TBD): Kernel Methods (Group 1 & 2)</li>
                <li><strong>TD 4</strong> (Date TBD): Neural Networks (Group 1 & 2)</li>
            </ul>

            <h2>TPs</h2>
            <ul>
                <li><strong>TP 1</strong> (Date TBD): TBD</li>
                <li><strong>TP 2</strong> (Date TBD): TBD</li>
                <li><strong>TP 3</strong> (Date TBD): TBD</li>
            </ul>

            <h2>References and External Resources</h2>

            <h3>Machine Learning and Learning Theory</h3>
            <ul>
                <li><a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target="_blank" rel="noopener noreferrer">Learning Theory from First Principles</a> by <a href="https://www.di.ens.fr/~fbach/" target="_blank" rel="noopener noreferrer">Francis Bach</a>, 2024: <strong>Main reference for the course.</strong></li>
                <li><a href="https://cs.nyu.edu/~mohri/mlbook/" target="_blank" rel="noopener noreferrer">Foundations of Machine Learning</a> by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, 2018.</li>
                <li><a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/index.html" target="_blank" rel="noopener noreferrer">Aurélien Garivier</a>'s <a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/index2baa.html?q=node/214" target="_blank" rel="noopener noreferrer">ML M2 class</a> at ENS Lyon: Complementary topics to those presented in this course.</li>
            </ul>

            <h3>Optimization for Machine Learning</h3>
            <ul>
                <li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener noreferrer">Convex Optimization</a> by Stephen Boyd and Lieven Vandenberghe, Cambridge University Press, 2012.</li>
                <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611974997" target="_blank" rel="noopener noreferrer">First-Order Optimization Methods</a> by Amir Beck, 2017.</li>
            </ul>

            <h3>Measure and Probability Theory</h3>
            <ul>
                <li><a href="https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/IPPA2.pdf" target="_blank" rel="noopener noreferrer">Intégration, Probabilités et Processus Aléatoires</a> by Jean-François Le Gall, 2006: An excellent introduction to measure theory with elements of stochastic processes and conditional expectation.</li>
                <li>Probabilités 2 by Jean-Yves Ouvrard, 2009: A detailed treatment of advanced topics in probability theory.</li>
                <li>Concentration Inequalities: A Nonasymptotic Theory of Independence by Stéphane Boucheron, Gábor Lugosi, and Pascal Massart, 2013.</li>
            </ul>

            <h3>Analysis</h3>
            <ul>
                <li><a href="http://www.cmap.polytechnique.fr/~massot/MAP431_web/MAP431_Histoire_Massot/Livres_Bony_Lutzen/Jean-Michel%20Bony-Cours%20d%27analyse.%20The%CC%81orie%20des%20distributions%20et%20analyse%20de%20Fourier-Ellipses(2001).pdf" target="_blank" rel="noopener noreferrer">Cours d'analyse</a> by Jean-Michel Bony, 2001.</li>
            </ul>

        </main>
    </div>
</body>
</html>
