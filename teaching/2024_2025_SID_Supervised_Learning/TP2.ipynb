{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPo3tLOLmghXhUtiwueDYYR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine Learning Notebook: Kernels, Kernel SVM and other fun stuff"],"metadata":{"id":"_Bzz83ORgS1z"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LinearRegression, Ridge\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.random_projection import GaussianRandomProjection\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import KFold\n","\n","# Set plot style\n","sns.set(style='whitegrid')"],"metadata":{"id":"Kgs0ZnJykDfz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Symmetric Positive Kernels"],"metadata":{"id":"iC7jpiTdrLeV"}},{"cell_type":"markdown","source":["Let $\\mathcal{X}$ be a feature space. We say that\n","$$K : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R} $$\n","is a symmetric positive kernel on $\\mathcal{X}$ when $\\forall n \\geq 1, \\forall x_1, \\dots, x_n \\in \\mathcal{X}$, the matrix $(K(x_i, x_j))_{1 \\leq i, j \\leq n} \\in \\mathcal{S}_n^{+} (\\mathbb{R})$\n","\n","When $n$ and $x_1, \\dots, x_n \\in \\mathcal{X}$ are fixed, the matrix $(K(x_i, x_j))_{1 \\leq i, j \\leq n}$ is usually refered to as the kernel matrix."],"metadata":{"id":"Fo0yDbXmpmQz"}},{"cell_type":"markdown","source":["The following block of code lets you vizualize the kernel matrices of two kernels in a toy example."],"metadata":{"id":"ws9bcAW5rvoj"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import polynomial_kernel, rbf_kernel, linear_kernel\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Toy dataset\n","X = np.random.randn(10, 2)\n","\n","# Compute kernel matrices\n","K_poly = polynomial_kernel(X, degree=3)\n","K_rbf = rbf_kernel(X, gamma=0.5)\n","\n","# Visualize the kernel matrices\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","sns.heatmap(K_poly, ax=axes[0], annot=True, cmap='viridis')\n","axes[0].set_title(\"Polynomial Kernel Matrix\")\n","sns.heatmap(K_rbf, ax=axes[1], annot=True, cmap='viridis')\n","axes[1].set_title(\"RBF Kernel Matrix\")\n","plt.show()"],"metadata":{"id":"RYVbDg6SkLVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question 1**\n","\n","Quickly justify why the functions polynomial_kernel and rbf_kernel in the previous code define symmetric positive kernels. You can use the sklearn documentation to obtain more information on those functions."],"metadata":{"id":"JF_aJfntopPt"}},{"cell_type":"markdown","source":["**Question 2**\n","\n","What property of the kernel matrices can we immediately see in the previous plots ?"],"metadata":{"id":"ONyUz7edr6iT"}},{"cell_type":"markdown","source":["## Kernel SVM"],"metadata":{"id":"h9Z0We6isZ9n"}},{"cell_type":"markdown","source":["Let $(x_1, y_1), \\dots, (x_n, y_n) \\in \\mathcal{X} \\times \\{-1, 1 \\}$, the point of the $2$-classes classification problem is to build a function $f$ such that\n","$$ f(x) \\approx y $$\n","on unseen data $(x, y)$.\n","\n","As we saw in class, linear SVM is a machine learning method that estimates a $2$-classes predictor by a function of the form\n","$$ f(x) = \\text{sgn} \\bigg( \\hat{\\theta}^T \\phi(x) \\bigg)$$\n","where\n","$$ \\hat{\\theta} \\in \\text{argmin}_{\\theta} \\frac{1}{n} \\sum_{i=1}^n (1 - y_i \\theta^T \\phi(x_i))_+ + \\lambda \\| \\theta\\|^2 \\;.$$\n","\n","In comparison, Kernel SVM is a machine learning method that estimates a $2$-classes predictor by a function of the form\n","$$ f(x) = \\text{sgn} \\bigg( \\alpha_1 K(x_1, x) + \\dots + \\alpha_n K(x_n, x) \\bigg)$$\n","where the $\\alpha$'s minimize the following expression :    \n","$$ \\sum_{i = 1}^n \\bigg(1 - y_i \\bigg( \\alpha_1 K(x_1, x_i) + \\dots + \\alpha_n K(x_n, x_i) \\bigg)\\bigg)_+ + \\lambda \\sum_{1 \\leq i, j \\leq n} \\alpha_i \\alpha_j K(x_i, x_j) \\;.$$"],"metadata":{"id":"IXph1clb51U4"}},{"cell_type":"markdown","source":["**Question 3**\n","\n","We denote by $H$ a Hilbert Space with Hilbert product $\\langle \\cdot, \\cdot \\rangle$ and by $\\phi : \\mathcal{X} \\rightarrow H$ the space and mapping given by Aronszajnâ€™s theorem such that\n","$$\\forall x_1, x_2 \\in \\mathcal{X}, \\quad k(x_1, x_2) = \\langle \\phi (x_1), \\phi(x_2) \\rangle \\;.$$\n","\n","Using the representer theorem, show that the previous expression for $f$ and the related problem for the $\\alpha$s is a consequence of the (in)finite-dimensional linear SVM problem\n","\n","$$ f(x) = \\text{sgn} \\bigg( \\langle \\hat{\\theta}, \\phi(x) \\rangle \\bigg)$$\n","where\n","$$ \\hat{\\theta} \\in \\text{argmin}_{\\theta \\in H} \\frac{1}{n} \\sum_{i=1}^n (1 - y_i\\langle \\theta, \\phi(x_i) \\rangle)_+ + \\lambda \\| \\theta\\|_H^2 \\;.$$\n","\n","In the end, what is the difference between a linear SVM and a kernel SVM ?"],"metadata":{"id":"bY6HhgRzKJJe"}},{"cell_type":"markdown","source":["The following code uses this method on a toy example called the \"two moons\" dataset. Warning : the two classes are encoded on $\\{0, 1 \\}$ instead of $\\{-1, 1 \\}$, remember to switch between the two encodings when needed."],"metadata":{"id":"aNf1F0Cl8Kil"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_moons\n","\n","# Generate a toy dataset\n","X, y = make_moons(n_samples=100, noise=0.1)\n","\n","# Train Kernel SVM\n","clf = SVC(kernel='rbf', C=1., gamma=0.5)\n","clf.fit(X, y)\n","\n","# Visualize the decision boundary\n","def plot_decision_boundary(model, X, y):\n","    h = .02\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n","    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)\n","    plt.show()\n","\n","plot_decision_boundary(clf, X, y)\n"],"metadata":{"id":"36xNDuD6nWQ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question 4**\n","\n","By modifying the previous cell (Copy-Paste the moddified versions below), present two examples, one with a clear underfitting situation, and one with a clear overfitting situation. The meaningful parametters to play with are the kernel type in SVC, the regularization parametter in SVC and the noise level in the data generation."],"metadata":{"id":"M1ViS_3wvEVA"}},{"cell_type":"code","source":["# Put the code for the underfitting example here"],"metadata":{"id":"otZY8FA6v2_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Put the code for the overfitting example here"],"metadata":{"id":"3_nE-KA2v3a3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Kernel SVM By Hand"],"metadata":{"id":"sYg4fG2OxJey"}},{"cell_type":"markdown","source":["Now that we have played with Kernel SVMs a bit, let's implement one ourselves (instead of using an already made library) !"],"metadata":{"id":"_Wv-36Vl9Mr2"}},{"cell_type":"markdown","source":["**Question 5**\n","\n","We will train by hand the kernel SVM model using Gradient Descent. Give the expression of the (sub)gradient of the objective gradient with respect to $\\alpha_1, \\dots, \\alpha_n$."],"metadata":{"id":"M7GWVmm6-jvy"}},{"cell_type":"markdown","source":["**Question 6**\n","\n","Complete the following script in order to implement your own Kernel SVM class."],"metadata":{"id":"anj5g7tR-w1k"}},{"cell_type":"code","source":["class SVM():\n","    \"\"\"\n","    Class to Handle SVMs.\n","    \"\"\"\n","    def __init__(self, kernel='linear', C=1., **kwargs):\n","        \"\"\"\n","        Initializes an SVM solver with regulizer constant C on the dual box constraint.\n","        \"\"\"\n","\n","\n","    def fit(self, X, y, precomputed_kernel_train=None):\n","        \"\"\"\n","        Fits the given data F(X)=y. You can pass a precomputed data kernel K(X, X).\n","        \"\"\"\n","\n","\n","    def predict(self, X, precomputed_kernel_train_eval=None):\n","        \"\"\"\n","        Predicts f(X). You can pass the precomputed kernel K(X_eval, X_train).\n","        \"\"\"\n"],"metadata":{"id":"9SsUPWjM93sQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Your code should work on the following examples :"],"metadata":{"id":"eHWkauilCQCf"}},{"cell_type":"code","source":["# Generate a toy dataset\n","X, y = make_moons(n_samples=100, noise=0.1)\n","\n","## Linear Kernel\n","\n","# Train Kernel SVM\n","clf = SVM(kernel='linear', C=1.)\n","clf.fit(X, y)\n","\n","# Visualize the decision boundary\n","plot_decision_boundary(clf, X, y)\n","\n","## Polynomial Kernel\n","\n","# Train Kernel SVM\n","clf = SVM(kernel=polynomial_kernel, C=1., degree=3)\n","clf.fit(X, y)\n","\n","# Visualize the decision boundary\n","plot_decision_boundary(clf, X, y)\n","\n","## Gaussian Kernel\n","\n","# Train Kernel SVM\n","clf = SVM(kernel=rbf_kernel, C=1., gamma=1)\n","clf.fit(X, y)\n","\n","# Visualize the decision boundary\n","plot_decision_boundary(clf, X, y)"],"metadata":{"id":"rKz495c6Amjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PjB6gWeNc962"},"execution_count":null,"outputs":[]}]}