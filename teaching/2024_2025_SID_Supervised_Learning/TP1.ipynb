{"cells":[{"cell_type":"markdown","metadata":{"id":"10aHNItMeis4"},"source":["# Machine Learning Notebook: Linear and Ridge Regression, Hyperparameter Tuning\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0ihwLsRdnjr"},"outputs":[],"source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LinearRegression, Ridge\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.random_projection import GaussianRandomProjection\n","from sklearn.datasets import make_regression, load_diabetes\n","from sklearn.model_selection import KFold\n","\n","# Set plot style\n","sns.set(style='whitegrid')"]},{"cell_type":"markdown","metadata":{"id":"p7FKTiOAe0TJ"},"source":["# 1. Introduction\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvjV0iSOet3I"},"outputs":[],"source":["# Generate a high-dimensional dataset\n","X, y = make_regression(n_samples=100, n_features=400, n_informative=1, noise=0.5, random_state=42)\n","\n","diabetes = load_diabetes()\n","X, y = diabetes.data, diabetes.target\n","\n","# Add a column of ones to X for the intercept\n","X = np.c_[np.ones((X.shape[0], 1)), X]\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"p7M_WEHgi6bU"},"source":["# Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"YqXhfYck-Rpp"},"source":["Linear regression is a method used to model the relationship between a dependent variable $y$ and one or more independent variables $X$. The model assumes a linear relationship between the inputs and the output:\n","\n","$$\n","y = X\\beta + \\epsilon\n","$$\n","\n","Where:\n","\n","- $X$ is the matrix of input features (with each row representing a data point).\n","    \n","- $\\beta$ are the coefficients (parameters) we want to estimate.\n","    \n","- $\\epsilon$ is the error term (assumed to be normally distributed).\n","\n","The goal of linear regression is to find the parameters $\\beta$ that minimize the sum of squared errors (SSE):\n","\n","$$\n","\\text{SSE} = \\sum_{i=1}^n (y_i - X_i \\beta)^2\n","$$\n","\n","This minimization problem is solved by computing the ordinary least squares (OLS) estimate:\n","\n","$$\n","\\hat{\\beta} = (X^T X)^{-1} X^T y\n","$$\n","\n","This equation gives the optimal parameters $\\beta$ that minimize the prediction error."]},{"cell_type":"markdown","metadata":{"id":"1yDUnmHsc3Bo"},"source":["**Question 1** : Implement the linear regression using the class sklearn.linear_model.LinearRegression."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lafapyTOGSiG"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"puieXmLtdYdd"},"source":["**Question 2** : Implement the linear regression by hand (using Numpy functions only)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOotFm-xTctO"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"AcTGP1YPANPf"},"source":["# Ridge Regression\n","\n","Ridge Regression is a regularized version of linear regression that addresses multicollinearity and prevents overfitting by adding a penalty term to the cost function. It modifies the linear regression objective by introducing an L2 regularization term to penalize large coefficients:\n","\n","$$\n","\\text{Ridge Cost Function} = \\sum_{i=1}^n (y_i - X_i \\beta)^2 + \\alpha \\sum_{j=1}^p \\beta_j^2\n","$$\n","\n","Where:\n","\n","- $\\alpha$ is the regularization parameter controlling the penalty strength.\n","    \n","- $\\beta_j$ are the regression coefficients.\n","\n","The second term $\\alpha \\sum_{j=1}^p \\beta_j^2$ discourages large values of $\\beta$, which helps prevent overfitting in high-dimensional or multicollinear datasets. The solution to the ridge regression is given by:\n","\n","$$\n","\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\alpha I)^{-1} X^T y\n","$$\n","\n","Where $I$ is the identity matrix. The addition of $\\alpha I$ ensures that the matrix is invertible, even in cases of multicollinearity."]},{"cell_type":"markdown","metadata":{"id":"AxMePwXZbJuW"},"source":["**Question 1** : Implement the Ridge regression using the function class sklearn.linear_model.Ridge."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePjFJ6OoGT1T"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"OUfGL6y-biNg"},"source":["**Question 2** : Implement the Ridge regression by hand (using Numpy functions only)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5UViPDFUvf8"},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"2-AwF9rUbw9L"},"source":["**Question 3** : Plot the train and test errors of the model as a function of $\\alpha$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThgMDXGMXjT9"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1laKX-ohGpoK"},"source":["# Hyperparameter Tuning 1 : Optimization over the valitation set\n","\n","Normal Validation (or train/validation split) is a common approach for evaluating a machine learning model. The dataset is split into two sets:\n","\n","- Training set: Used to train the model.\n","\n","- Validation set: Used to evaluate the model's performance on unseen data.\n","\n","Mathematically, this can be represented as:\n","\n","$$\n","X_{\\text{train}}, y_{\\text{train}} \\quad \\text{and} \\quad X_{\\text{val}}, y_{\\text{val}}\n","$$\n","\n","The model is trained on $(X_{\\text{train}}, y_{\\text{train}})$ and evaluated on $(X_{\\text{val}}, y_{\\text{val}})$. This process helps detect overfitting because the model is tested on data that it hasnâ€™t seen during training.\n","\n","The performance metric (e.g., mean squared error) is calculated on the validation set:\n","\n","$$\n","\\text{MSE}_{\\text{val}} = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} (y_{\\text{val}_i} - \\hat{y}_{\\text{val}_i})^2\n","$$\n","\n","This score is then used in hyperparameter tuning and model selection."]},{"cell_type":"markdown","metadata":{"id":"tPgpsg1SsTdu"},"source":["**Question** : Split the train set in a smaller train set and a validation set and then tune the Ridge parametter on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrVvUFifQJrA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"RPb7TFSeQLsi"},"source":["# Hyperparameter Tuning 2 : Cross-Validation\n","\n","Cross-Validation is a technique used to ensure that the model generalizes well to unseen data (more complex than regular validation). The most common form is $k$-fold cross-validation, where the dataset is split into $k$ subsets (folds). The model is trained on $k-1$ folds and evaluated on the remaining fold. This process is repeated $k$ times, with each fold serving as the validation set once.\n","\n","Mathematically, for each fold $i$:\n","\n","- Train the model on $k-1$ folds\n","\n","- Validate the model on the $i$-th fold\n","\n","The performance metric (e.g., MSE) is computed for each fold, and the average score (other statistics such as the median, percentiles, ... may be used alternatively) is calculated:\n","\n","$$\n","\\text{MSE}_{\\text{cv}} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{MSE}_{\\text{val}_i}\n","$$\n","\n","Cross-validation helps in reducing the variability of the validation scores and ensures the model is tested on multiple subsets of data, leading to more robust model selection.\n"]},{"cell_type":"markdown","metadata":{"id":"TJYKWfOXxV5g"},"source":["**Question :** Implement cross-validation using KFold from sklearn.model_selection and use it to determine a good projection dimension with random projections."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"505s-2mfRNoa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfE5M6O8yN7z"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUOiWDlT8ZcU4l4aZNPAGu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}