<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematics of Machine Learning M2 MAPI3 (2024-2025)</title>
    <link rel="stylesheet" href="../../css/styles.css">
</head>
<body>
    <div class="container">
        <main class="main">
            <div id="toptitle">
                <h1>Mathematics of Machine Learning M2 MAPI3 (Toulouse University, 2025-2026)</h1>
                <strong>Lecturers:</strong> <a href="../../index.html" target="_blank" rel="noopener noreferrer">Clément Lalanne</a>, Valentin Lafargue
            </div>
            <div id="content">
                <h2>Overview</h2>
                <p>This class aims to introduce the main theoretical foundations of modern machine learning, with a focus on supervised learning. Basic knowledge of linear algebra and probability theory (including measure theory) is required. For the last lectures, basic knowledge of functional analysis is recommended.</p>
            </div>

            
            <h2>Evaluation</h2>

            For all students, the final grade will consist of an 80% weight from a 2-hour final exam and 20% from in-class work. For part-time working students (étudiants en alternance), the in-class grade will be based on three MCQs (QCMs en Français). For other students, the in-class grade will be split equally: 50% from MCQs and 50% from the <a href="../2025_2026_MAPI3_Machine_Learning_Projects/index.html" target="_blank" rel="noopener noreferrer">projects</a>.</p><br>

            QCM 1 : 13/10/2025 sur les quatre premiers cours. QCM 2 : 05/11/2025 TPs et méthodes expérimentales.

            <div style="
                color: red;
                font-size: 1.8em;
                font-weight: bold;
                text-align: center;
                background-color: #ffecec;
                border: 2px solid red;
                border-radius: 10px;
                padding: 15px;
                margin-top: 20px;
            ">
            ⚠️ AVIS IMPORTANT ⚠️ <br>
            En raison de l’indisponibilité de plusieurs étudiants pour le QCM du <strong>5 novembre</strong>, 
            celui-ci est reporté au <u>12 janvier</u>.
            </div>
            
            

            <h3>Survival Kit for the Exam</h3>
            <p>For the exam, you should be familiar with the following concepts and techniques:</p>
            <ul>
                <li>Decomposing risk as an estimation error (considering the uniform worst-case error over the predictor set), approximation error, and potential other error sources (e.g., optimization errors). You should be able to recognize the different bias and variance terms in this decomposition.</li>
                <li>Basic linear algebra (vectors, linear maps, rank, matrices) and bilinear algebra (eigenvectors, eigenvalues, spectral theorem, basic matrix decompositions).</li>
                <li>Multivariable calculus (Gradients, Hessians, Taylor expansions, higher-order derivative tensors), Convexity, First and Second Order optimality conditions, KKT conditions.</li>
                <li>Basic probability theory in general probability spaces, conditional laws, conditional expectations, and independence.</li>
                <li>Common inequalities: Triangle inequality, convexity, AM-GM inequality, Cauchy-Schwarz, Jensen, Hölder, Minkowski.</li>
                <li>Concentration inequalities: Markov, Bienaymé-Tchebychev and McDarmid's inequality.</li>
                <li>Rademacher complexity: Definition, upper bounding the sup deviations (each side by two times the Rademacher complexity), Lipschitz contraction principle, and typical use cases.</li>
                <li>Kernel methods: Aronszajn's theorem (equivalence between positive kernels and Hilbert space dot products), representer theorem, kernel trick, operations on kernels, and Bochner's theorem.</li>
                <li>Optimization: general optimization priciples and usual regularity assumptions.</li>
            </ul>

            
            <h2>Lectures</h2>
            <ul>
                <li><strong>Lecture 1</strong> <a href="MachineLearningMAPI3Lecture1Introduction.pdf" target="_blank" rel="noopener noreferrer">Introduction to supervised learning</a>.</li>
                <li><strong>Lecture 2</strong> <a href="MachineLearningMAPI3Lecture2RegressionsLinearRidge.pdf" target="_blank" rel="noopener noreferrer">Linear regression and Ridge regression</a>.</li>
                <li><strong>Lecture 3</strong> <a href="MachineLearningMAPI3Lecture3KernelMethods.pdf" target="_blank" rel="noopener noreferrer">Kernel methods</a>.</li>
                <li><strong>Lecture 4</strong> <a href="MachineLearningMAPI3Lecture3KernelMethods.pdf" target="_blank" rel="noopener noreferrer">Kernel methods continued</a> (<a href="https://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/notes/aronszajn.pdf" target="_blank" rel="noopener noreferrer">Jean-Philippe Vert's proof of Aronszajn's theorem</a>).</li>
                <li><strong>Lecture 5</strong> <a href="MachineLearningMAPI3Lecture5ERM1.pdf" target="_blank" rel="noopener noreferrer">ERM 1</a>.</li>
                <li><strong>Lecture 6</strong> <a href="MachineLearningMAPI3Lecture6ERM2.pdf" target="_blank" rel="noopener noreferrer">ERM 2</a>.</li>
                <li><strong>Lecture 7</strong> <a href="MachineLearningMAPI3Lecture7Optimization.pdf" target="_blank" rel="noopener noreferrer">Optimization for Machine Learning</a>.</li>
                <li><strong>Lecture 8</strong> </li>
                <li><strong>Lecture 9</strong> </li>
            </ul>

            <h2>TDs / TPs</h2>
            <ul>
                <li><strong>TD/TP 1</strong> <a href="TP1.ipynb" target="_blank" rel="noopener noreferrer">Linear Regression, Ridge Regression, and (Cross-)Validation</a>  (<a href="TP1_correction.ipynb" target="_blank" rel="noopener noreferrer">solution</a>)</li>
                <li><strong>TD/TP 2</strong> <a href="TD_Logistic.pdf" target="_blank" rel="noopener noreferrer">Logistic Regression</a> (<a href="Correction_TD_Logistic.pdf" target="_blank" rel="noopener noreferrer">solution</a>).</li>
                <li><strong>TD/TP 3</strong> <a href="TD_Kernel.pdf" target="_blank" rel="noopener noreferrer">Kernel Methods</a> (<a href="Correction_TD_Kernel.pdf" target="_blank" rel="noopener noreferrer">solution</a>).</li>
                <li><strong>TD/TP 4</strong> </li>
                <li><strong>TD/TP 5</strong> </li>
                <li><strong>TD/TP 6</strong> </li>
            </ul>

            <h2>References and External Resources</h2>

            <h3>Machine Learning and Learning Theory</h3>
            <ul>
                <li><a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target="_blank" rel="noopener noreferrer">Learning Theory from First Principles</a> by <a href="https://www.di.ens.fr/~fbach/" target="_blank" rel="noopener noreferrer">Francis Bach</a>, 2024: <strong>Main reference for the course.</strong></li>
                <li><a href="https://arxiv.org/abs/2404.17625" target="_blank" rel="noopener noreferrer">Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land</a> by Simone Scardapane, 2024: A very accessible hands on presentation of modern machine learning methods.</li> 
                <li><a href="https://cs.nyu.edu/~mohri/mlbook/" target="_blank" rel="noopener noreferrer">Foundations of Machine Learning</a> by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, 2018.</li>
                <li><a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/index.html" target="_blank" rel="noopener noreferrer">Aurélien Garivier</a>'s <a href="https://perso.ens-lyon.fr/aurelien.garivier/www.math.univ-toulouse.fr/_agarivie/index2baa.html?q=node/214" target="_blank" rel="noopener noreferrer">ML M2 class</a> at ENS Lyon: Complementary topics to those presented in this course.</li>
            </ul>

            <h3>Generative AI</h3>
            <ul>
                <li><a href="https://www.youtube.com/@Deepia-ls2fo" target="_blank" rel="noopener noreferrer">Deepia's Youtube channel.</li>
                <li><a href="https://arxiv.org/pdf/2403.18103" target="_blank" rel="noopener noreferrer">Tutorial on Diffusion Models for Imaging and Vision</a> by Stanley Chan, 2024.</li>
                <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank" rel="noopener noreferrer">What are diffusion models ?</a> by Lilian Weng, 2021.</li>
                <li><a href="https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/" target="_blank" rel="noopener noreferrer">A Visual Dive into Conditional Flow Matching</a> by Gagneux et al., 2025.</li>
            </ul>

            <h3>Optimization for Machine Learning</h3>
            <ul>
                <li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener noreferrer">Convex Optimization</a> by Stephen Boyd and Lieven Vandenberghe, Cambridge University Press, 2012.</li>
                <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611974997" target="_blank" rel="noopener noreferrer">First-Order Optimization Methods</a> by Amir Beck, 2017.</li>
                <li><a href="https://arxiv.org/pdf/2101.09545" target="_blank" rel="noopener noreferrer">Acceleration Methods</a> by Alexandre d’Aspremont, Damien Scieur and Adrien Taylor, 2024.</li>
            </ul>

            <h3>Measure and Probability Theory</h3>
            <ul>
                <li><a href="https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/IPPA2.pdf" target="_blank" rel="noopener noreferrer">Intégration, Probabilités et Processus Aléatoires</a> by Jean-François Le Gall, 2006: An excellent introduction to measure theory with elements of stochastic processes and conditional expectation.</li>
                <li>Probabilités 2 by Jean-Yves Ouvrard, 2009: A detailed treatment of advanced topics in probability theory.</li>
                <li>Concentration Inequalities: A Nonasymptotic Theory of Independence by Stéphane Boucheron, Gábor Lugosi, and Pascal Massart, 2013.</li>
            </ul>

            <h3>Analysis</h3>
            <ul>
                <li><a href="http://www.cmap.polytechnique.fr/~massot/MAP431_web/MAP431_Histoire_Massot/Livres_Bony_Lutzen/Jean-Michel%20Bony-Cours%20d%27analyse.%20The%CC%81orie%20des%20distributions%20et%20analyse%20de%20Fourier-Ellipses(2001).pdf" target="_blank" rel="noopener noreferrer">Cours d'analyse</a> by Jean-Michel Bony, 2001.</li>
            </ul>

        </main>
    </div>
</body>
</html>
